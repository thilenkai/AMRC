---
title: "Exercise 1"
subtitle: "Advanced Methods for Regression and Classification"
author: "Kai Thilenius"
date: "10/14/2025"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 0. Load and show data

```{r, echo=TRUE}
 data(College,package="ISLR")
 str(College)
 # Check for missing data
 print(paste("Number of missing values:", sum(is.na(College))))
```

## 1. Predicting *Outstate* response by only using *Expend* as predictor

*Outstate*: Out-of-state tuition - how much a college charges students from outside the state

*Expend:* Instructional expenditure per student - how much the college spends on instruction for each student

```{r, echo=TRUE}

# linear regression based on those two only
lm1 <- lm(Outstate ~ Expend, data = College)

# Plot the data, and visualize the regression line
plot(College$Expend, College$Outstate,
     xlab = "Expend ($ per student)",
     ylab = "Outstate tuition ($)",
     main = "Outstate vs Expend",
     pch = 19, col = "blue")

abline(lm1, col = "red", lwd = 2)
```

**Conclusion**:

-   positive estimated slope - so the more the college pays per student, the higher the tuition

-   BUT some "outliers" towards the right (way higher expends per student, than usual) give the predicted line a bias

## 2. More appropriate model against bias

As the plot before showed a few single points far away from the estimation, getting rid of those "outliers" would be the first idea to make the model "more appropriate". A boxplot of this one variable *Expend* helps visualizing this situation and also shows, that the **outliers are all above** the maxium. The common way of cutting both the low and the high quartile might be too drastic, so we **only take out the quartile of the highest expends**.

```{r, echo=TRUE}
# verifying outlier theory
boxplot(College$Expend,
        main = "Boxplot of Expend",
        ylab = "Expend ($ per student)")
```

```{r, echo=TRUE}

# Calculate quartiles and IQR
Q1 <- quantile(College$Expend, 0.25)
Q3 <- quantile(College$Expend, 0.75)
IQR <- Q3 - Q1

# Keep all rows except extreme high outliers
College_no_outliers <- subset(College, Expend < (Q3 + 1.5 * IQR))

# calculate new model after removing upper outliers
lm2 <- lm(Outstate ~ Expend, data = College_no_outliers)

```

Then we can compare both models visually and numerically (see table below). We seem to see a more appropriate 2nd model. We will continue using this "clened" dataset in the following questions:

```{r, echo=TRUE}
par(mfrow = c(1, 2))

# Original model
plot(College$Expend, College$Outstate,
     main = "Original Data",
     xlab = "Expend", ylab = "Outstate",
     pch = 19, col = "gray")
abline(lm1, col = "red", lwd = 2)

# After removing high outliers
plot(College_no_outliers$Expend, College_no_outliers$Outstate,
     main = "After Removing Upper Outliers",
     xlab = "Expend", ylab = "Outstate",
     pch = 19, col = "gray")
abline(lm2, col = "blue", lwd = 2)

par(mfrow = c(1, 1))

## Values copied in LateX table
# summary(lm1)$r.squared
# summary(lm2)$r.squared

# summary(lm1)$coefficients
# summary(lm2)$coefficients

```

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
\hline
\textbf{Metric} & \textbf{Original Model} & \textbf{No-Outliers Model} \\
\hline
$R^2$ & 0.4526 & 0.5156 \\
Intercept & 5433.51 & 1939.38 \\
Slope (Expend) & 0.518 & 0.927 \\
$p$-value (Expend) & $< 2 \times 10^{-16}$ & $< 2 \times 10^{-16}$ \\
\hline
\end{tabular}
\caption{Comparison of linear regression models predicting \textit{Outstate} from \textit{Expend}, before and after removing upper outliers.}
\label{tab:model_comparison}
\end{table}

## 3. Predict response on *Apps* with binary variable *Private*

```{r, echo=TRUE}
lm3 <- lm(Apps ~ Private, data = College_no_outliers)
summary(lm3)

```

The intercept of 5729.9 here is the predicted average number of applications for colleges where *Private = "No"*. The *Private = "Yes"* signifies the difference in average applications between private and public colleges and is with -4108.1 negative, so: Private colleges=5729.9-4108.1=1621.8 applications (on average). As they both have small p-values, their differece is statistically significant.

## 4.Convert **Private** as variable with level +/- 1

The leveling the following ruleset is expected:

\begin{table}[h!]
\centering
\begin{tabular}{c|c|c}
     & -1 & +1 \\
\hline
No   & 1  & 0  \\
Yes  & 0  & 1
\end{tabular}
\caption{Your table caption here}
\end{table}

With this transformation the intercept now represents the overall mean of the Data.

```{r, echo=TRUE}
College_no_outliers$Private_leveled <- ifelse(College_no_outliers$Private == "Yes", 1, -1)

table(College_no_outliers$Private, College_no_outliers$Private_leveled)

lm4 <- lm(Apps ~ Private_leveled, data = College_no_outliers)
summary(lm4)

```

## 5. Predict **Apps** response by all variables

Doing that only makes sense with variables that make sense. This "content-wise" excludes:

-   Accept: is determined by **Apps** and can only be affects after the application happened

-   Enroll: also depends on the application process

-   Personal: college internal spendings

The other variables might "content-wise" influence applicants by having an effect on the colleges reputation, popularity, costs, ... and we get some good looking graphs Diagnostic Plots:

```{r, echo=TRUE}
set.seed(123)   # same randomness seed for reproducibility
n <- nrow(College_no_outliers)
train_index <- sample(1:n, size = 2/3 * n)
train <- College_no_outliers[train_index, ]
test <- College_no_outliers[-train_index, ]

lm5 <- lm(Apps ~ Private + Top10perc + Top25perc + F.Undergrad +
                P.Undergrad + Outstate + Room.Board + Books + PhD + 
                Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate,
              data = train)

# summary(lm5)

par(mfrow = c(2, 2)) # arrange in a 2x2 grid
plot(lm5)

# pairs(College_no_outliers[, c("Apps", "F.Undergrad", "Expend", "Grad.Rate", "Outstate")]) # for pairwise relationship between vaiables - not asked here

```

## 6. Same model, scaled variables

The standardized coefficients allow direct comparison of predictor importance:

```{r, echo=TRUE}
lm6 <- lm(Apps ~ scale(Top10perc) + scale(Top25perc) + scale(F.Undergrad) +
                        scale(P.Undergrad) + scale(Outstate) + scale(Room.Board) +
                        scale(Books) + scale(PhD) + scale(Terminal) +
                        scale(S.F.Ratio) + scale(perc.alumni) + scale(Expend) +
                        scale(Grad.Rate) + Private,
                 data = train)
summary(lm6)

```

Because variables are standardized, we see that **F.Undergrad** has by far the **largest effect** on `Apps`, followed by **Room.Board**, **Grad.Rate**, and **Expend**.

## 7. RMSEs of 5. and 6. and their comparison

with the formula $$
\text{RMSE} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2}
$$ we can compare the performace of the previous models.

```{r, echo=TRUE}
# --- Predictions for lm5 (unscaled model) ---
pred_train_lm5 <- predict(lm5, newdata = train)
pred_test_lm5  <- predict(lm5, newdata = test)

# --- Compute RMSE for lm5 ---
rmse_train_lm5 <- sqrt(mean((train$Apps - pred_train_lm5)^2))
rmse_test_lm5  <- sqrt(mean((test$Apps - pred_test_lm5)^2))

# --- Predictions for lm6 (scaled model) ---
pred_train_lm6 <- predict(lm6, newdata = train)
pred_test_lm6  <- predict(lm6, newdata = test)

# --- Compute RMSE for lm6 ---
rmse_train_lm6 <- sqrt(mean((train$Apps - pred_train_lm6)^2))
rmse_test_lm6  <- sqrt(mean((test$Apps - pred_test_lm6)^2))

# --- Combine results into a table ---
rmse_results <- data.frame(
  Model = c("lm5 (unscaled)", "lm6 (scaled)"),
  RMSE_Train = c(rmse_train_lm5, rmse_train_lm6),
  RMSE_Test  = c(rmse_test_lm5, rmse_test_lm6)
)

rmse_results

```

Well ... something went wrong obvioulsy. You should not trust (Chat GPT) on that one ...

## 8. Lets see if we get any further with the log-transformed response

```{r, echo=TRUE}
lm8 <- lm(log(Apps) ~ Private + Top10perc + Top25perc + F.Undergrad +
                        P.Undergrad + Outstate + Room.Board + Books + PhD + 
                        Terminal + S.F.Ratio + perc.alumni + Expend + Grad.Rate,
              data = train)
par(mfrow = c(2, 2))
plot(lm8)


```

Diagnostic plots look good actually!

## 9. Yeah ... obviously we can not compare the RMSEs

Though i actually tried it lol. Here is a better solution probably - the Akaikeâ€™s information criterion:

```{r, echo=TRUE}
AIC(lm5, lm8)

```

AIC of lm8 (which is the log transformed model) is way better than of lm5, so it is performing better!
