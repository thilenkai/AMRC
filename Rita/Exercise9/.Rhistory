scope = list(
lower = ~1,  # Start with no predictors
upper = ~ s(chol, k = 5) + s(stab.glu, k = 5) +
s(age, k = 4) + s(bmi, k = 5) +
s(bp.1s, k = 5) + location + gender + frame
))
?gam::step.Gam  # Open help documentation
# Load the necessary libraries
library(gam)
library(mgcv)
# Initial GAM model with all variables
initial_gam <- gam(dtest ~ s(chol, k = 5) + s(stab.glu, k = 5) +
s(age, k = 4) + s(bmi, k = 5) +
s(bp.1s, k = 5) + location + gender + frame,
data = train_data, family = "binomial")
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
library(ROCit)
library(glmnet)
library(mgcv)
library(caret)
library(dplyr)
# Load the Diabetes data
data(Diabetes, package = "ROCit")
# Exclude variables with high missingness
excluded_vars <- c("id", "bp.2s", "bp.2d")
diabetes_data <- Diabetes %>% select(-all_of(excluded_vars))
# Impute missing values for `frame` using mode imputation
mode_impute <- function(x) {
levels(x)[which.max(tabulate(match(x, levels(x))))]
}
diabetes_data$frame[is.na(diabetes_data$frame)] <-
mode_impute(diabetes_data$frame)
# Remove rows with any missing values
diabetes_data <- na.omit(diabetes_data)
# Verify the updated dataset
cat("Remaining rows after removing missing values:",
nrow(diabetes_data), "\n")
cat("Remaining columns:", ncol(diabetes_data), "\n")
summary(diabetes_data)
# Check for class imbalance in target variable
table(diabetes_data$dtest)
# Select only numeric predictors
numeric_data <- diabetes_data %>% select(-dtest) %>% select_if(is.numeric)
# Compute the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")
cat("Correlation Matrix:\n")
print(cor_matrix)
# Visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method = "circle", type = "upper",
tl.cex = 0.8, main = "Correlation Matrix")
# Identify highly correlated pairs (correlation > 0.7 but < 1)
high_corr <- which(abs(cor_matrix) > 0.7 & abs(cor_matrix) < 1, arr.ind = TRUE)
cat("Highly Correlated Predictor Pairs:\n")
print(high_corr)
# Drop redundant predictors
# Keep 'bmi' (drop 'weight', 'waist', 'hip'),
# keep 'bp.1s' (drop 'bp.1d'), keep 'stab.glu' (drop 'glyhb')
diabetes_data <- diabetes_data %>%
select(-waist, -hip, -bp.1d, -glyhb, -ratio, -weight)
# Verify remaining predictors
cat("Remaining predictors after removing highly correlated ones:\n")
print(names(diabetes_data))
# Update the training and test splits after removing missing values
set.seed(123)  # For reproducibility
train_index <- createDataPartition(diabetes_data$dtest, p = 0.75, list = FALSE)
train_data <- diabetes_data[train_index, ]
test_data <- diabetes_data[-train_index, ]
# Print the sizes of the training and test sets
cat("Training set size after removing missing values:", nrow(train_data), "\n")
cat("Test set size after removing missing values:", nrow(test_data), "\n")
# Ensure dtest is a factor (0 and 1)
train_data$dtest <- ifelse(train_data$dtest == "+", 1, 0)
test_data$dtest <- ifelse(test_data$dtest == "+", 1, 0)
train_data$dtest <- as.factor(train_data$dtest)
test_data$dtest <- as.factor(test_data$dtest)
# Fit a logistic regression model using the training set
logistic_model <- glm(dtest ~ ., data = train_data, family = "binomial")
# Summary of the model
cat("Logistic Regression Summary:\n")
print(summary(logistic_model))
# Predict probabilities on the test set
test_predictions <- predict(logistic_model,
newdata = test_data, type = "response")
# Convert probabilities to class labels (0 or 1)
test_class_predictions <- ifelse(test_predictions > 0.5, 1, 0)
# Confusion matrix
confusion_matrix <- table(Predicted = test_class_predictions, Actual = test_data$dtest)
cat("Confusion Matrix:\n")
print(confusion_matrix)
# Calculate misclassification rate
misclassification_rate <- mean(test_class_predictions != test_data$dtest)
cat("Misclassification Rate:", misclassification_rate, "\n")
# Prepare data for glmnet (requires matrix for predictors)
x_train <- model.matrix(dtest ~ ., data = train_data)[, -1]  # Remove intercept
y_train <- as.numeric(as.character(train_data$dtest))  # Ensure y is numeric (0/1)
x_test <- model.matrix(dtest ~ ., data = test_data)[, -1]
y_test <- as.numeric(as.character(test_data$dtest))   # Ensure test labels are numeric
# Fit Lasso (alpha = 1) with cross-validation
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
# Plot cross-validation curve
plot(cv_lasso)
cat("Optimal lambda:", cv_lasso$lambda.min, "\n")
# Refit the model using optimal lambda
lasso_model <- glmnet(x_train, y_train, family = "binomial",
alpha = 1, lambda = cv_lasso$lambda.min)
# Display coefficients
lasso_coef <- coef(lasso_model)
cat("Non-zero coefficients (Lasso):\n")
print(lasso_coef)
# Predict probabilities on the test set
lasso_prob <- predict(lasso_model, s = cv_lasso$lambda.min,
newx = x_test, type = "response")
# Convert probabilities to binary predictions (0 or 1)
lasso_pred <- ifelse(lasso_prob > 0.5, 1, 0)
# Ensure predictions and actual values are integers for comparison
lasso_pred <- as.integer(lasso_pred)
y_test <- as.integer(y_test)
# Confusion Matrix
lasso_conf_matrix <- table(Predicted = lasso_pred, Actual = y_test)
cat("Confusion Matrix (Lasso):\n")
print(lasso_conf_matrix)
# Misclassification Rate
lasso_misclass_rate <- sum(lasso_pred != y_test) / length(y_test)
cat("Misclassification Rate (Lasso):", round(lasso_misclass_rate, 4), "\n")
# Load the necessary package
library(mgcv)
# Fit a GAM model (using only linear terms for all predictors)
gam_model <- gam(dtest ~ chol + stab.glu + age + bmi +
location + gender + frame,
data = train_data, family = "binomial")
# Summary of the model
summary(gam_model)
# Predict on test data
gam_predictions <- predict(gam_model, newdata = test_data,
type = "response")
# Convert probabilities to class labels
gam_class_predictions <- ifelse(gam_predictions > 0.5, 1, 0)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM without Smoothness):",
gam_misclass_rate, "\n")
# Fit the GAM model
gam_model <- gam(dtest ~ s(chol) + s(stab.glu) + s(age) + s(bmi) + s(bp.1s) +
location + gender + frame,
data = train_data, family = "binomial")
# Summary of the GAM model
cat("GAM Model Summary:\n")
print(summary(gam_model))
# Predict on the test set
gam_predictions <- predict(gam_model, newdata = test_data, type = "response")
# Convert probabilities to class labels
gam_class_predictions <- ifelse(gam_predictions > 0.5, 1, 0)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
cat("Confusion Matrix (GAM):\n")
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM):", gam_misclass_rate, "\n")
# Fit the GAM model with limited degrees of freedom for smooth terms
gam_model <- gam(dtest ~ s(chol, k = 5) + s(stab.glu, k = 5) + s(age, k = 4) +
s(bmi, k = 5) + s(bp.1s, k = 5) +
location + gender + frame,
data = train_data, family = "binomial")
# Summary of the GAM model
cat("GAM Model Summary with Limited Degrees of Freedom:\n")
print(summary(gam_model))
# Predict on the test set
gam_predictions <- predict(gam_model, newdata = test_data, type = "response")
# Convert probabilities to class labels
gam_class_predictions <- ifelse(gam_predictions > 0.5, 1, 0)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
cat("Confusion Matrix (GAM with Limited Degrees of Freedom):\n")
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM with Limited Degrees of Freedom):",
gam_misclass_rate, "\n")
# Plot smoothed terms with shaded confidence intervals
plot(gam_model, page = 1, shade = TRUE, shade.col = "yellow", seWithMean = TRUE)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
cat("Confusion Matrix (GAM with Limited Degrees of Freedom):\n")
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM with Limited Degrees of Freedom):",
gam_misclass_rate, "\n")
remove.packages("gam")
install.packages("gam", dependencies = TRUE)
library(gam)
library(gam)
??gam
help("gam", package = "gam")
conflicts()
find("step.gam")
sessionInfo()
ls("package:gam")
gam::step.gam
library(gam)
??gam
help("gam", package = "gam")
conflicts()
find("step.gam")
sessionInfo()
ls("package:gam")
# Load the necessary libraries
library(gam)
library(mgcv)
# Initial GAM model with all variables
initial_gam <- gam(dtest ~ s(chol, k = 5) + s(stab.glu, k = 5) +
s(age, k = 4) + s(bmi, k = 5) +
s(bp.1s, k = 5) + location + gender + frame,
data = train_data, family = "binomial")
conflicts()
find("gam")
find("s")
# Load the necessary libraries
library(mgcv)  # Ensure mgcv is loaded
library(gam)   # Still loaded, but we'll force mgcv functions
# Explicitly use mgcv's gam and s functions
initial_gam <- mgcv::gam(dtest ~ mgcv::s(chol, k = 5) +
mgcv::s(stab.glu, k = 5) +
mgcv::s(age, k = 4) +
mgcv::s(bmi, k = 5) +
mgcv::s(bp.1s, k = 5) +
location + gender + frame,
data = train_data, family = "binomial")
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
library(ROCit)
library(glmnet)
library(mgcv)
library(caret)
library(dplyr)
# Load the Diabetes data
data(Diabetes, package = "ROCit")
# Exclude variables with high missingness
excluded_vars <- c("id", "bp.2s", "bp.2d")
diabetes_data <- Diabetes %>% select(-all_of(excluded_vars))
# Impute missing values for `frame` using mode imputation
mode_impute <- function(x) {
levels(x)[which.max(tabulate(match(x, levels(x))))]
}
diabetes_data$frame[is.na(diabetes_data$frame)] <-
mode_impute(diabetes_data$frame)
# Remove rows with any missing values
diabetes_data <- na.omit(diabetes_data)
# Verify the updated dataset
cat("Remaining rows after removing missing values:",
nrow(diabetes_data), "\n")
cat("Remaining columns:", ncol(diabetes_data), "\n")
summary(diabetes_data)
# Check for class imbalance in target variable
table(diabetes_data$dtest)
# Select only numeric predictors
numeric_data <- diabetes_data %>% select(-dtest) %>% select_if(is.numeric)
# Compute the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")
cat("Correlation Matrix:\n")
print(cor_matrix)
# Visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method = "circle", type = "upper",
tl.cex = 0.8, main = "Correlation Matrix")
# Identify highly correlated pairs (correlation > 0.7 but < 1)
high_corr <- which(abs(cor_matrix) > 0.7 & abs(cor_matrix) < 1, arr.ind = TRUE)
cat("Highly Correlated Predictor Pairs:\n")
print(high_corr)
# Drop redundant predictors
# Keep 'bmi' (drop 'weight', 'waist', 'hip'),
# keep 'bp.1s' (drop 'bp.1d'), keep 'stab.glu' (drop 'glyhb')
diabetes_data <- diabetes_data %>%
select(-waist, -hip, -bp.1d, -glyhb, -ratio, -weight)
# Verify remaining predictors
cat("Remaining predictors after removing highly correlated ones:\n")
print(names(diabetes_data))
# Update the training and test splits after removing missing values
set.seed(123)  # For reproducibility
train_index <- createDataPartition(diabetes_data$dtest, p = 0.75, list = FALSE)
train_data <- diabetes_data[train_index, ]
test_data <- diabetes_data[-train_index, ]
# Print the sizes of the training and test sets
cat("Training set size after removing missing values:", nrow(train_data), "\n")
cat("Test set size after removing missing values:", nrow(test_data), "\n")
# Ensure dtest is a factor (0 and 1)
train_data$dtest <- ifelse(train_data$dtest == "+", 1, 0)
test_data$dtest <- ifelse(test_data$dtest == "+", 1, 0)
train_data$dtest <- as.factor(train_data$dtest)
test_data$dtest <- as.factor(test_data$dtest)
# Fit a logistic regression model using the training set
logistic_model <- glm(dtest ~ ., data = train_data, family = "binomial")
# Summary of the model
cat("Logistic Regression Summary:\n")
print(summary(logistic_model))
# Predict probabilities on the test set
test_predictions <- predict(logistic_model,
newdata = test_data, type = "response")
# Convert probabilities to class labels (0 or 1)
test_class_predictions <- ifelse(test_predictions > 0.5, 1, 0)
# Confusion matrix
confusion_matrix <- table(Predicted = test_class_predictions, Actual = test_data$dtest)
cat("Confusion Matrix:\n")
print(confusion_matrix)
# Calculate misclassification rate
misclassification_rate <- mean(test_class_predictions != test_data$dtest)
cat("Misclassification Rate:", misclassification_rate, "\n")
# Prepare data for glmnet (requires matrix for predictors)
x_train <- model.matrix(dtest ~ ., data = train_data)[, -1]  # Remove intercept
y_train <- as.numeric(as.character(train_data$dtest))  # Ensure y is numeric (0/1)
x_test <- model.matrix(dtest ~ ., data = test_data)[, -1]
y_test <- as.numeric(as.character(test_data$dtest))   # Ensure test labels are numeric
# Fit Lasso (alpha = 1) with cross-validation
set.seed(123)
cv_lasso <- cv.glmnet(x_train, y_train, family = "binomial", alpha = 1)
# Plot cross-validation curve
plot(cv_lasso)
cat("Optimal lambda:", cv_lasso$lambda.min, "\n")
# Refit the model using optimal lambda
lasso_model <- glmnet(x_train, y_train, family = "binomial",
alpha = 1, lambda = cv_lasso$lambda.min)
# Display coefficients
lasso_coef <- coef(lasso_model)
cat("Non-zero coefficients (Lasso):\n")
print(lasso_coef)
# Predict probabilities on the test set
lasso_prob <- predict(lasso_model, s = cv_lasso$lambda.min,
newx = x_test, type = "response")
# Convert probabilities to binary predictions (0 or 1)
lasso_pred <- ifelse(lasso_prob > 0.5, 1, 0)
# Ensure predictions and actual values are integers for comparison
lasso_pred <- as.integer(lasso_pred)
y_test <- as.integer(y_test)
# Confusion Matrix
lasso_conf_matrix <- table(Predicted = lasso_pred, Actual = y_test)
cat("Confusion Matrix (Lasso):\n")
print(lasso_conf_matrix)
# Misclassification Rate
lasso_misclass_rate <- sum(lasso_pred != y_test) / length(y_test)
cat("Misclassification Rate (Lasso):", round(lasso_misclass_rate, 4), "\n")
# Load the necessary package
library(mgcv)
# Fit a GAM model (using only linear terms for all predictors)
gam_model <- gam(dtest ~ chol + stab.glu + age + bmi +
location + gender + frame,
data = train_data, family = "binomial")
# Summary of the model
summary(gam_model)
# Predict on test data
gam_predictions <- predict(gam_model, newdata = test_data,
type = "response")
# Convert probabilities to class labels
gam_class_predictions <- ifelse(gam_predictions > 0.5, 1, 0)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM without Smoothness):",
gam_misclass_rate, "\n")
# Fit the GAM model
gam_model <- gam(dtest ~ s(chol) + s(stab.glu) + s(age) + s(bmi) + s(bp.1s) +
location + gender + frame,
data = train_data, family = "binomial")
# Summary of the GAM model
cat("GAM Model Summary:\n")
print(summary(gam_model))
# Predict on the test set
gam_predictions <- predict(gam_model, newdata = test_data, type = "response")
# Convert probabilities to class labels
gam_class_predictions <- ifelse(gam_predictions > 0.5, 1, 0)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
cat("Confusion Matrix (GAM):\n")
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM):", gam_misclass_rate, "\n")
# Fit the GAM model with limited degrees of freedom for smooth terms
gam_model <- gam(dtest ~ s(chol, k = 5) + s(stab.glu, k = 5) + s(age, k = 4) +
s(bmi, k = 5) + s(bp.1s, k = 5) +
location + gender + frame,
data = train_data, family = "binomial")
# Summary of the GAM model
cat("GAM Model Summary with Limited Degrees of Freedom:\n")
print(summary(gam_model))
# Predict on the test set
gam_predictions <- predict(gam_model, newdata = test_data, type = "response")
# Convert probabilities to class labels
gam_class_predictions <- ifelse(gam_predictions > 0.5, 1, 0)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
cat("Confusion Matrix (GAM with Limited Degrees of Freedom):\n")
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM with Limited Degrees of Freedom):",
gam_misclass_rate, "\n")
# Plot smoothed terms with shaded confidence intervals
plot(gam_model, page = 1, shade = TRUE, shade.col = "yellow", seWithMean = TRUE)
# Confusion matrix
gam_conf_matrix <- table(Predicted = gam_class_predictions,
Actual = test_data$dtest)
cat("Confusion Matrix (GAM with Limited Degrees of Freedom):\n")
print(gam_conf_matrix)
# Misclassification rate
gam_misclass_rate <- mean(gam_class_predictions != test_data$dtest)
cat("Misclassification Rate (GAM with Limited Degrees of Freedom):",
gam_misclass_rate, "\n")
library(gam)
??gam
help("gam", package = "gam")
conflicts()
find("step.gam")
sessionInfo()
ls("package:gam")
conflicts()
find("gam")
find("s")
# Load the necessary libraries
library(mgcv)  # Ensure mgcv is loaded
library(gam)   # Still loaded, but we'll force mgcv functions
# Explicitly use mgcv's gam and s functions
initial_gam <- mgcv::gam(dtest ~ mgcv::s(chol, k = 5) +
mgcv::s(stab.glu, k = 5) +
mgcv::s(age, k = 4) +
mgcv::s(bmi, k = 5) +
mgcv::s(bp.1s, k = 5) +
location + gender + frame,
data = train_data, family = "binomial")
# Explicitly use mgcv functions to avoid conflicts with gam
initial_gam <- mgcv::gam(dtest ~ mgcv::s(chol, k = 5) +
mgcv::s(stab.glu, k = 5) +
mgcv::s(age, k = 4) +
mgcv::s(bmi, k = 5) +
mgcv::s(bp.1s, k = 5) +
location + gender + frame,
data = train_data, family = "binomial")
# Start with all variables
current_formula <- dtest ~ s(chol) + s(stab.glu) + s(age) + s(bmi) +
s(bp.1s) + location + gender + frame
# Fit the initial model
initial_gam <- gam(current_formula, data = train_data, family = "binomial")
cat("Initial Model Summary:\n")
print(summary(initial_gam))
# Step 1: Identify insignificant variables
# Look at p-values and smooth terms for manual decisions
insignificant_vars <- c("age", "bmi", "location", "gender", "frame") # Example from summaries
# Step 2: Iteratively drop variables and refit
for (var in insignificant_vars) {
cat("\nDropping:", var, "\n")
# Update formula by removing the variable
current_formula <- update(current_formula, paste(". ~ . -", var))
# Refit the model
updated_gam <- mgcv::gam(current_formula, data = train_data, family = "binomial")
print(summary(updated_gam))
# Predict on test data
gam_pred <- predict(updated_gam, newdata = test_data, type = "response")
gam_class <- ifelse(gam_pred > 0.5, 1, 0)
# Evaluate performance
conf_matrix <- table(Predicted = gam_class, Actual = test_data$dtest)
cat("Confusion Matrix:\n")
print(conf_matrix)
misclass_rate <- mean(gam_class != test_data$dtest)
cat("Misclassification Rate:", misclass_rate, "\n")
# Decide to keep or drop the variable based on misclassification rate
if (misclass_rate > 0.2) {  # Example threshold
cat(var, "seems important, re-adding it back.\n")
current_formula <- update(current_formula, paste(". ~ . +", var))
}
}
# Final Model
cat("\nFinal Model Summary:\n")
final_gam <- mgcv::gam(current_formula, data = train_data, family = "binomial")
print(summary(final_gam))
library(gam)
# ??gam
# help("gam", package = "gam")
# conflicts()
# find("step.gam")
# sessionInfo()
# ls("package:gam")
# Fit a GAM with selected variables from (f)
selected_formula <- dtest ~ s(chol) + s(stab.glu) + s(age) + s(bmi) + s(bp.1s)
selected_gam <- mgcv::gam(selected_formula, data = train_data, family = "binomial")
# Summary of the updated model
cat("Summary of GAM with Selected Variables:\n")
print(summary(selected_gam))
# Predict on the test set
selected_gam_pred <- predict(selected_gam, newdata = test_data, type = "response")
selected_gam_class <- ifelse(selected_gam_pred > 0.5, 1, 0)
# Confusion matrix
conf_matrix <- table(Predicted = selected_gam_class, Actual = test_data$dtest)
cat("Confusion Matrix:\n")
print(conf_matrix)
# Misclassification rate
misclass_rate <- mean(selected_gam_class != test_data$dtest)
cat("Misclassification Rate (Selected GAM):", misclass_rate, "\n")
