n <- 12
# Total number of possible bootstrap samples
total_bootstrap_samples <- n^n
# Print the result
total_bootstrap_samples
# Original sample data points
original_sample <-
c(4.94, 5.06, 4.53, 5.07, 4.99, 5.16, 4.38, 4.43, 4.93, 4.72, 4.92, 4.96)
original_mean <- mean(original_sample)
original_median <- median(original_sample)
list(mean = original_mean, median = original_median)
# Number of bootstrap samples
n_bootstrap <- 2000
# Generate 2000 bootstrap samples and compute their means
bootstrap_means <- replicate(n_bootstrap,
mean(sample(original_sample, replace = TRUE)))
# Display the first few bootstrap means
head(bootstrap_means)
# Compute the mean of the first 20 bootstrap means
mean_first_20 <- mean(bootstrap_means[1:20])
# Display the result
mean_first_20
# Compute the mean of the first 200 bootstrap means
mean_first_200 <- mean(bootstrap_means[1:200])
# Display the result
mean_first_200
# Compute the mean based on all 2000 bootstrap means
mean_all_2000 <- mean(bootstrap_means)
# Display the result
mean_all_2000
# Visualize the distribution of bootstrap means compared to the original mean
hist(bootstrap_means,
main = "Distribution of Bootstrap Means",
xlab = "Bootstrap Means",
breaks = 30,
col = "lightblue",
border = "darkblue")
abline(v = mean(original_sample),
col = "red",
lwd = 2,
lty = 2)
legend("topright",
legend = c("Sample Mean"),
col = c("red"),
lwd = 2,
lty = 2)
# Compute the 0.025 and 0.975 quantiles for the bootstrap means
quantiles_20 <- quantile(bootstrap_means[1:20], c(0.025, 0.975))
quantiles_200 <- quantile(bootstrap_means[1:200], c(0.025, 0.975))
quantiles_2000 <- quantile(bootstrap_means, c(0.025, 0.975))
# Compute the "true" confidence interval for the mean using t.test
true_ci <- t.test(original_sample)$conf.int
# Display all intervals
list(
quantiles_20 = quantiles_20,
quantiles_200 = quantiles_200,
quantiles_2000 = quantiles_2000,
true_ci = true_ci
)
# Number of bootstrap samples
n_bootstrap <- 2000
# Generate 2000 bootstrap samples and compute their medians
bootstrap_medians <- replicate(n_bootstrap,
median(sample(original_sample, replace = TRUE)))
# Display the first few bootstrap medians
head(bootstrap_medians)
# Compute the mean of the first 20 bootstrap medians
mean_first_20_medians <- mean(bootstrap_medians[1:20])
mean_first_20_medians
# Compute the mean of the first 200 bootstrap medians
mean_first_200_medians <- mean(bootstrap_medians[1:200])
mean_first_200_medians
# Compute the mean based on all 2000 bootstrap medians
mean_all_2000_medians <- mean(bootstrap_medians)
mean_all_2000_medians
# Visualize the distribution of bootstrap medians compared to the original median
hist(bootstrap_medians,
main = "Distribution of Bootstrap Medians",
xlab = "Bootstrap Medians",
breaks = 30,
col = "lightgreen",
border = "darkgreen")
abline(v = median(original_sample),
col = "blue",
lwd = 2,
lty = 2)  # Add a vertical line for the original sample median
legend("topright",
legend = c("Sample Median"),
col = c("blue"),
lwd = 2,
lty = 2)
# Compute 0.025 and 0.975 quantiles for the first 20 bootstrap medians
quantiles_median_20 <- quantile(bootstrap_medians[1:20], c(0.025, 0.975))
# Compute 0.025 and 0.975 quantiles for the first 200 bootstrap medians
quantiles_median_200 <- quantile(bootstrap_medians[1:200], c(0.025, 0.975))
# Compute 0.025 and 0.975 quantiles for all 2000 bootstrap medians
quantiles_median_2000 <- quantile(bootstrap_medians, c(0.025, 0.975))
list(
quantiles_median_20 = quantiles_median_20,
quantiles_median_200 = quantiles_median_200,
quantiles_median_2000 = quantiles_median_2000
)
# Set seed for reproducibility
set.seed(1234)
# Sample 1960 points from a standard normal distribution
x.clean <- rnorm(1960)
# Sample 40 observations from a uniform distribution between 4 and 5
x.cont <- runif(40, min = 4, max = 5)
# Combine the clean data with the contaminated data
x <- c(x.clean, x.cont)
# Set seed to the immatriculation number
set.seed(12332281)
# Display summary information about the data
list(
x_clean_summary = summary(x.clean),
x_cont_summary = summary(x.cont),
combined_summary = summary(x)
)
# Compute statistics for x.clean (clean data)
median_clean <- median(x.clean)
mean_clean <- mean(x.clean)
trimmed_mean_clean <- mean(x.clean, trim = 0.05)
# Compute statistics for x (combined data with outliers)
median_all <- median(x)
mean_all <- mean(x)
trimmed_mean_all <- mean(x, trim = 0.05)  # Trimmed mean with alpha = 0.05
list(
clean = list(
median = median_clean,
mean = mean_clean,
trimmed_mean = trimmed_mean_clean
),
combined = list(
median = median_all,
mean = mean_all,
trimmed_mean = trimmed_mean_all
)
)
# Function to calculate bootstrap statistics
bootstrap_stats <- function(data, func, n_bootstrap = 2000) {
bootstrap_samples <- replicate(n_bootstrap,
func(sample(data, replace = TRUE)))
se <- sd(bootstrap_samples)  # Standard error
ci <- quantile(bootstrap_samples, c(0.025, 0.975))
list(se = se, ci = ci)
}
# Nonparametric bootstrap for x.clean
median_clean_bootstrap <- bootstrap_stats(x.clean, median)
mean_clean_bootstrap <- bootstrap_stats(x.clean, mean)
trimmed_mean_clean_bootstrap <- bootstrap_stats(x.clean,
function(x) mean(x, trim = 0.05))
# Nonparametric bootstrap for x (combined data)
median_all_bootstrap <- bootstrap_stats(x, median)
mean_all_bootstrap <- bootstrap_stats(x, mean)
trimmed_mean_all_bootstrap <- bootstrap_stats(x,
function(x) mean(x, trim = 0.05))
# Display results
list(
clean = list(
median = median_clean_bootstrap,
mean = mean_clean_bootstrap,
trimmed_mean = trimmed_mean_clean_bootstrap
),
combined = list(
median = median_all_bootstrap,
mean = mean_all_bootstrap,
trimmed_mean = trimmed_mean_all_bootstrap
)
)
# Function to perform parametric bootstrap
parametric_bootstrap <- function(data, func, dist_func, params, n_bootstrap = 2000) {
# Bootstrap estimates
bootstrap_samples <- replicate(n_bootstrap, func(do.call(dist_func, params)))
# Bias calculation
original_estimate <- func(data)
bootstrap_mean <- mean(bootstrap_samples)
bias <- bootstrap_mean - original_estimate
# Standard error
se <- sd(bootstrap_samples)
# 95% percentile confidence interval
ci <- quantile(bootstrap_samples, c(0.025, 0.975))
# Bias-corrected estimate
bias_corrected_estimate <- original_estimate - bias
list(
bias = bias,
se = se,
ci = ci,
bias_corrected_estimate = bias_corrected_estimate
)
}
# Parametric bootstrap for x.clean
params_clean <- list(mean = mean(x.clean), sd = mad(x.clean), n = length(x.clean))
mean_clean_param <- parametric_bootstrap(x.clean, mean, rnorm, params_clean)
trimmed_mean_clean_param <- parametric_bootstrap(x.clean, function(x) mean(x, trim = 0.05), rnorm, params_clean)
# Parametric bootstrap for x (combined data)
params_combined <- list(mean = mean(x), sd = mad(x), n = length(x))
mean_combined_param <- parametric_bootstrap(x, mean, rnorm, params_combined)
trimmed_mean_combined_param <- parametric_bootstrap(x, function(x) mean(x, trim = 0.05), rnorm, params_combined)
# Display results
list(
clean = list(
mean = mean_clean_param,
trimmed_mean = trimmed_mean_clean_param
),
combined = list(
mean = mean_combined_param,
trimmed_mean = trimmed_mean_combined_param
)
)
# Data for plotting bias
bias_data <- data.frame(
Statistic = c("Mean", "Trimmed Mean", "Mean", "Trimmed Mean"),
Data = c("Clean", "Clean", "Combined", "Combined"),
Bias = c(-0.00032, -0.00410, -0.00110, 0.04729)
)
# Plot bias
library(ggplot2)
ggplot(bias_data, aes(x = Statistic, y = Bias, fill = Data)) +
geom_bar(stat = "identity", position = "dodge") +
labs(title = "Bias Comparison", y = "Bias", x = "Statistic") +
theme_minimal()
# Data for plotting CI
ci_data <- data.frame(
Statistic = rep(c("Mean", "Trimmed Mean"), each = 4),
Data = rep(c("Clean", "Combined"), times = 4),
Bound = rep(c("Lower", "Upper"), each = 2, times = 2),
CI = c(-0.04862, 0.03530, -0.04865, 0.03832, 0.04039, 0.12786, 0.03955, 0.12611)
)
# Plot CI
ggplot(ci_data, aes(x = Statistic, y = CI, fill = Data)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~ Bound, scales = "free_y") +
labs(title = "95% Confidence Interval Comparison", y = "CI Bounds", x = "Statistic") +
theme_minimal()
# Example: Non-parametric Bootstrap CI for the Mean
data <- rnorm(100)  # Example data
B <- 1000           # Number of bootstrap samples
# Generate bootstrap estimates
boot_means <- replicate(B, mean(sample(data, replace = TRUE)))
# Compute 95% Confidence Interval
ci <- quantile(boot_means, probs = c(0.025, 0.975))
cat("Non-Parametric Bootstrap CI for Mean:", ci, "\n")
# Example: Parametric Bootstrap CI for the Mean
data <- rnorm(100, mean = 5, sd = 2)  # Example data
params <- list(mean = mean(data), sd = sd(data))
B <- 1000
# Generate bootstrap estimates
boot_means <- replicate(B, mean(rnorm(length(data), params$mean, params$sd)))
# Compute 95% Confidence Interval
ci <- quantile(boot_means, probs = c(0.025, 0.975))
cat("Parametric Bootstrap CI for Mean:", ci, "\n")
# Example: Non-Parametric Hypothesis Test for the Mean
data <- rnorm(50, mean = 1)  # Example data
B <- 1000
observed_mean <- mean(data)
# Shift data to simulate the null hypothesis (mean = 0)
boot_means <- replicate(B, mean(sample(data - observed_mean, replace = TRUE)))
# Compute p-value
p_value <- mean(abs(boot_means) >= abs(observed_mean))
cat("P-value (Non-Parametric):", p_value, "\n")
# Example: Parametric Hypothesis Test for the Mean
data <- rnorm(50, mean = 1)  # Example data
null_mean <- 0
B <- 1000
# Generate bootstrap samples under the null
boot_means <- replicate(B, mean(rnorm(length(data), mean = null_mean, sd = sd(data))))
observed_mean <- mean(data)
# Compute p-value
p_value <- mean(abs(boot_means) >= abs(observed_mean))
cat("P-value (Parametric):", p_value, "\n")
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
library(ROCit)
# Generate measureit object
measure_result <- measureit(
score = train_predictions,  # Predicted probabilities from the model
class = train_data$Status,  # Actual class labels
measure = c("TPR", "TNR"),  # Sensitivity (TPR) and Specificity (TNR)
cutoff = seq(0, 1, by = 0.01)  # Sequence of cutoff values
)
# Extract Balanced Accuracy (TPR + TNR) / 2
balanced_accuracy <- (measure_result$TPR + measure_result$TNR) / 2
# Find the optimal cutoff value
optimal_index <- which.max(balanced_accuracy)
optimal_cutoff <- measure_result$Cutoff[optimal_index]
optimal_cutoff
# Generate measureit object
measure_result <- measureit(
score = train_predictions,  # Predicted probabilities from the model
class = train_data$Status,  # Actual class labels
measure = c("TPR", "TNR"),  # Sensitivity (TPR) and Specificity (TNR)
cutoff = seq(0, 1, by = 0.01)  # Sequence of cutoff values
)
# Extract Balanced Accuracy (TPR + TNR) / 2
balanced_accuracy <- (measure_result$TPR + measure_result$TNR) / 2
# Find the optimal cutoff value
optimal_index <- which.max(balanced_accuracy)
optimal_cutoff <- measure_result$Cutoff[optimal_index]
optimal_cutoff
# Plot Balanced Accuracy vs. Cutoff
plot(
measure_result$Cutoff, balanced_accuracy,
type = "l", col = "blue", lwd = 2,
xlab = "Cutoff Value", ylab = "Balanced Accuracy",
main = "Balanced Accuracy vs. Cutoff"
)
# Highlight the optimal cutoff
points(optimal_cutoff, balanced_accuracy[optimal_index], col = "red", pch = 19)
text(optimal_cutoff, balanced_accuracy[optimal_index],
labels = paste("Optimal Cutoff =", round(optimal_cutoff, 2)), pos = 4, col = "red")
# Predict on the test set using the optimal cutoff
optimal_cutoff <- 0.82
test_predictions <- predict(lm_model, newdata = test_data)
test_predicted_classes <- ifelse(test_predictions > optimal_cutoff, 1, 0)
# Create the confusion matrix
confusion_matrix <- table(
Actual = test_data$Status,
Predicted = test_predicted_classes
)
# Display the confusion matrix
print(confusion_matrix)
# Calculate performance metrics
tp <- confusion_matrix[2, 2]
tn <- confusion_matrix[1, 1]
fp <- confusion_matrix[1, 2]
fn <- confusion_matrix[2, 1]
# Sensitivity (TPR)
sensitivity <- tp / (tp + fn)
# Specificity (TNR)
specificity <- tn / (tn + fp)
# Balanced Accuracy
balanced_accuracy <- (sensitivity + specificity) / 2
# Print the metrics
cat("Sensitivity (TPR):", sensitivity, "\n")
cat("Specificity (TNR):", specificity, "\n")
cat("Balanced Accuracy:", balanced_accuracy, "\n")
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
# Load necessary libraries
if (!requireNamespace("ROCit", quietly = TRUE)) {
install.packages("ROCit")
}
if (!requireNamespace("pROC", quietly = TRUE)) {
install.packages("pROC")
}
library(ROCit)
library(pROC)
# Load the Loan dataset
data("Loan", package = "ROCit")
# View the dataset's structure
str(Loan)
head(Loan)
summary(Loan)
# Check levels of the Status variable
levels(Loan$Status)
# Recode factor levels explicitly (if "CO" should be 0 and "FP" should be 1)
Loan$Status <- as.numeric(factor(Loan$Status, levels = c("CO", "FP"))) - 1
# Check the resulting values
table(Loan$Status)
# Check for missing values
sum(is.na(Loan))
# Remove rows with missing values
Loan <- na.omit(Loan)
# Check for zero or negative values in Income
sum(Loan$Income <= 0)  # Should return 0 for valid data
# Identify numerical columns
num_cols <- c("Amount", "Income", "IntRate", "ILR", "Score")
# Standardize the numerical columns
Loan[, num_cols] <- scale(Loan[, num_cols])
# Confirm scaling
summary(Loan[, num_cols])
max_outlier <- Loan[which(scale(Loan$Income) > 9), "Income"]
max_outlier  # Check the exact value
# Winsorize Income at 3 standard deviations
Loan$Income <- ifelse(Loan$Income > 3, 3, Loan$Income)
summary(Loan[, c("Amount", "IntRate", "ILR", "Score")])
# Set a seed for reproducibility
set.seed(12332281)
# Split data into training and test sets
train_indices <- sample(1:nrow(Loan), size = 2/3 * nrow(Loan))
train_data <- Loan[train_indices, ]
test_data <- Loan[-train_indices, ]
# Build the linear regression model
lm_model <- lm(Status ~ ., data = train_data)
summary(lm_model)
plot(lm_model)
# Predict the response for the training set
train_predictions <- predict(lm_model, newdata = train_data)
# Visualize predictions vs. true class labels
plot(
train_data$Status,        # True class labels on the x-axis
train_predictions,        # Predicted values on the y-axis
xlab = "True Class Labels (Status)",
ylab = "Predicted Values",
main = "Predicted vs. True Class Labels",
col = "blue",
pch = 19
)
# Add a horizontal line at cutoff 0.9
abline(h = 0.8, col = "red", lty = 2)
# Add jitter to true class labels to make the scatter clearer
points(jitter(train_data$Status), train_predictions, col = "darkgreen", pch = 19)
# Define a sequence of cutoff values
cutoffs <- seq(0, 1, by = 0.01)
# Initialize vectors to store TPR, FPR, and Balanced Accuracy
tpr <- fpr <- balanced_accuracy <- numeric(length(cutoffs))
# Loop through each cutoff and calculate metrics
for (i in seq_along(cutoffs)) {
predicted_class <- ifelse(train_predictions > cutoffs[i], 1, 0)
# Confusion matrix components
tp <- sum(predicted_class == 1 & train_data$Status == 1)
tn <- sum(predicted_class == 0 & train_data$Status == 0)
fp <- sum(predicted_class == 1 & train_data$Status == 0)
fn <- sum(predicted_class == 0 & train_data$Status == 1)
# Calculate TPR, FPR, and Balanced Accuracy
tpr[i] <- tp / (tp + fn)
fpr[i] <- fp / (fp + tn)
balanced_accuracy[i] <- (tpr[i] + (1 - fpr[i])) / 2
}
# Identify the best cutoff value
best_cutoff <- cutoffs[which.max(balanced_accuracy)]
best_cutoff
# Plot the ROC Curve
roc_curve <- roc(train_data$Status, train_predictions)
plot(roc_curve, main = "ROC Curve", col = "blue", lwd = 2)
# Highlight the best cutoff on the ROC curve
points(1 - fpr[which.max(balanced_accuracy)], tpr[which.max(balanced_accuracy)],
col = "red", pch = 19)
legend("bottomright", legend = paste("Best Cutoff =", round(best_cutoff, 2)),
col = "red", pch = 19)
# Use the best cutoff value (e.g., 0.82)
best_cutoff <- 0.82
# Predict class labels based on the best cutoff
train_predicted <- ifelse(train_predictions > best_cutoff, 1, 0)
# Create the confusion matrix
confusion_matrix <- table(Actual = train_data$Status, Predicted = train_predicted)
print(confusion_matrix)
# Calculate metrics
tp <- confusion_matrix[2, 2]  # True Positives
tn <- confusion_matrix[1, 1]  # True Negatives
fp <- confusion_matrix[1, 2]  # False Positives
fn <- confusion_matrix[2, 1]  # False Negatives
tpr <- tp / (tp + fn)  # Sensitivity / Recall
tnr <- tn / (tn + fp)  # Specificity
balanced_accuracy <- (tpr + tnr) / 2
cat("Sensitivity (TPR):", tpr, "\n")
cat("Specificity (TNR):", tnr, "\n")
cat("Balanced Accuracy:", balanced_accuracy, "\n")
# Apply rocit() function using training predictions and actual labels
roc_result <- rocit(score = train_predictions, class = train_data$Status)
# Display summary of the ROC result
summary(roc_result)
# Plot the ROC curve
plot(roc_result,
main = "ROC Curve for Model",
col = "blue",
lwd = 2)
# Add AUC legend to the plot
legend("topleft",
legend = paste("AUC =", round(roc_result$AUC, 2)),
col = "blue",
lwd = 2)
# Generate measureit object
measure_result <- measureit(
score = train_predictions,  # Predicted probabilities from the model
class = train_data$Status,  # Actual class labels
measure = c("TPR", "TNR"),  # Sensitivity (TPR) and Specificity (TNR)
cutoff = seq(0, 1, by = 0.01)  # Sequence of cutoff values
)
# Extract Balanced Accuracy (TPR + TNR) / 2
balanced_accuracy <- (measure_result$TPR + measure_result$TNR) / 2
# Find the optimal cutoff value
optimal_index <- which.max(balanced_accuracy)
optimal_cutoff <- measure_result$Cutoff[optimal_index]
optimal_cutoff
# Plot Balanced Accuracy vs. Cutoff
plot(
measure_result$Cutoff, balanced_accuracy,
type = "l", col = "blue", lwd = 2,
xlab = "Cutoff Value", ylab = "Balanced Accuracy",
main = "Balanced Accuracy vs. Cutoff"
)
# Highlight the optimal cutoff
points(optimal_cutoff, balanced_accuracy[optimal_index], col = "red", pch = 19)
text(optimal_cutoff, balanced_accuracy[optimal_index],
labels = paste("Optimal Cutoff =", round(optimal_cutoff, 2)), pos = 4, col = "red")
# Predict on the test set using the optimal cutoff
optimal_cutoff <- 0.82
test_predictions <- predict(lm_model, newdata = test_data)
test_predicted_classes <- ifelse(test_predictions > optimal_cutoff, 1, 0)
# Create the confusion matrix
confusion_matrix <- table(
Actual = test_data$Status,
Predicted = test_predicted_classes
)
# Display the confusion matrix
print(confusion_matrix)
# Calculate performance metrics
tp <- confusion_matrix[2, 2]
tn <- confusion_matrix[1, 1]
fp <- confusion_matrix[1, 2]
fn <- confusion_matrix[2, 1]
# Sensitivity (TPR)
sensitivity <- tp / (tp + fn)
# Specificity (TNR)
specificity <- tn / (tn + fp)
# Balanced Accuracy
balanced_accuracy <- (sensitivity + specificity) / 2
# Print the metrics
cat("Sensitivity (TPR):", sensitivity, "\n")
cat("Specificity (TNR):", specificity, "\n")
cat("Balanced Accuracy:", balanced_accuracy, "\n")
