---
title: "Exercise 7"
subtitle: "Advanced Methods for Regression and Classification"
author: "Rita Selimi"
date: "03/12/2024"
output: 
  pdf_document:
    latex_engine: xelatex
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
```

# Task 1

## Load and Explore the Data

```{r load-data}
# Load the data
d <- read.csv2("bank.csv")

# Exclude the variable 'duration'
d <- d[, !colnames(d) %in% "duration"]

# Convert target variable to a factor
d$y <- as.factor(d$y)

# Check class distribution
table(d$y)

head(d)
summary(d)
```

The dataset contains information about direct marketing campaigns
conducted by a Portuguese bank to predict whether clients subscribe to a
term deposit (binary target `y`, with 4000 "no" and 521 "yes"). It
includes 16 variables, such as client demographics (e.g., `age`, `job`,
`marital`), financial data (`balance`, `housing`, `loan`), and campaign
details (`contact`, `campaign`, `pdays`). The dataset is heavily
imbalanced, with many more "no" responses, and key variables like
`balance`, `previous`, and `pdays` may strongly influence predictions.

## (a) Logistic Regression on Training Set

```{r logistic-regression}
set.seed(123)

# Randomly select 3000 observations for training
train_indices <- sample(nrow(d), 3000)
train_data <- d[train_indices, ]
test_data <- d[-train_indices, ]

# Fit logistic regression model
logit_model <- glm(y ~ ., data = train_data, family = "binomial")

# Summary of the model
summary(logit_model)
```

The logistic regression analysis reveals key factors influencing whether
clients subscribe to a term deposit. The most significant predictor is
the success of previous campaigns (poutcomesuccess), strongly increasing
the likelihood of subscription. Other important variables include
specific months, such as October (positive effect) and May (negative
effect), and certain client characteristics like being retired.
Additionally, using unknown contact methods or being married has a
significant impact on the outcome. The reduction in deviance from 2153.3
(null model) to 1761.4 (model with predictors) shows that the regression
model significantly improves prediction accuracy by using the provided
variables, such as age, job, and contact method. The lower AIC value of
1845.4 indicates a good balance between model fit and simplicity.

## (b) Predictions and Evaluation on Test Set

```{r predictions}
# Predict probabilities for the test set
test_data$predicted_prob <- predict(logit_model, newdata = test_data, 
                                    type = "response")

# Convert probabilities to class labels (default threshold = 0.5)
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, "yes", "no")

# Create a confusion matrix
conf_matrix <- table(Predicted = test_data$predicted_class, 
                     Actual = test_data$y)

# Calculate misclassification rate for each group
misclass_rate_no <- conf_matrix["yes", "no"] / sum(conf_matrix[, "no"])
misclass_rate_yes <- conf_matrix["no", "yes"] / sum(conf_matrix[, "yes"])

# Calculate balanced accuracy
TPR <- conf_matrix["yes", "yes"] / sum(conf_matrix[, "yes"]) # True Positive Rate
TNR <- conf_matrix["no", "no"] / sum(conf_matrix[, "no"]) # True Negative Rate
balanced_accuracy <- (TPR + TNR) / 2

# Print results
cat("Misclassification Rate for 'no':", misclass_rate_no, "\n")
cat("Misclassification Rate for 'yes':", misclass_rate_yes, "\n")
cat("Balanced Accuracy:", balanced_accuracy, "\n")
```

The model is highly biased toward predicting "no", which aligns with the
majority class in this imbalanced dataset. While it correctly identifies
"no" with high accuracy, it performs poorly for "yes", the minority
class. This suggests that further improvements, such as rebalancing the
dataset, are needed to improve the model's ability to identify "yes"
clients effectively. The balanced accuracy score of 59.15% is slightly
better than random guessing (50%), but it’s not strong.

## (c) Weighted Logistic Regression

**Assign Weights Inversely Proportional to Class Frequencies**

The idea is to give more weight to the minority class (`yes`) and less
weight to the majority class (`no`). This is done using the formula:

$$
\text{Weight} = \frac{1}{\text{Frequency of the Class}}
$$

For example: - If there are 500 `yes` and 3500 `no` observations: -
Weight for `yes` = $\frac{1}{500}$ - Weight for `no` = $\frac{1}{3500}$

```{r weighted-logistic-regression}
# Calculate weights
class_weights <- ifelse(train_data$y == "yes", 
                        1 / sum(train_data$y == "yes"), 
                        1 / sum(train_data$y == "no"))

# Normalize the weights
class_weights <- class_weights * length(train_data$y)

# Fit weighted logistic regression model
weighted_logit_model <- glm(y ~ ., data = train_data, family = "binomial", 
                            weights = class_weights)

# Predict probabilities on the test set
test_data$weighted_prob <- predict(weighted_logit_model, newdata = test_data, 
                                   type = "response")

# Convert probabilities to class labels (default threshold = 0.5)
test_data$weighted_class <- ifelse(test_data$weighted_prob > 0.5, "yes", "no")

# Create a confusion matrix
conf_matrix_weighted <- table(Predicted = test_data$weighted_class, 
                              Actual = test_data$y)

# Calculate misclassification rate for each group
misclass_rate_no_weighted <- 
  conf_matrix_weighted["yes", "no"] / sum(conf_matrix_weighted[, "no"])
misclass_rate_yes_weighted <- 
  conf_matrix_weighted["no", "yes"] / sum(conf_matrix_weighted[, "yes"])

# Calculate balanced accuracy
TPR_weighted <- 
  conf_matrix_weighted["yes", "yes"] / sum(conf_matrix_weighted[, "yes"]) 
TNR_weighted <- 
  conf_matrix_weighted["no", "no"] / sum(conf_matrix_weighted[, "no"])    
balanced_accuracy_weighted <- (TPR_weighted + TNR_weighted) / 2

# Print results
cat("Misclassification Rate for 'no' (weighted):", 
    misclass_rate_no_weighted, "\n")
cat("Misclassification Rate for 'yes' (weighted):", 
    misclass_rate_yes_weighted, "\n")
cat("Balanced Accuracy (weighted):", balanced_accuracy_weighted, "\n")
```

Adding weights to the logistic regression model reduced the
misclassification rate for the minority class ("yes") but increased the
error rate for the majority class ("no"). The balanced accuracy showed a
slight improvement (59.15% → 66.19%).

Normalizing the weights ensures that the total weight matches the
dataset size, keeping the model output consistent.

## (d) Stepwise Variable Selection

```{r stepwise-selection}
# Perform stepwise selection on the weighted logistic regression model
simplified_model <- step(weighted_logit_model, direction = "both")

# Summarize the simplified model
summary(simplified_model)

# Predict probabilities using the simplified model on the test set
test_data$simplified_prob <- predict(simplified_model, newdata = test_data, 
                                     type = "response")

# Convert probabilities to class labels (default threshold = 0.5)
test_data$simplified_class <- ifelse(test_data$simplified_prob > 0.5, "yes", "no")

# Ensure confusion matrix includes both "no" and "yes"
conf_matrix_simplified <- table(
  Predicted = factor(test_data$simplified_class, levels = c("no", "yes")),
  Actual = factor(test_data$y, levels = c("no", "yes"))
)

# Re-run misclassification rate calculations
misclass_rate_no_simplified <- 
  conf_matrix_simplified["yes", "no"] / sum(conf_matrix_simplified[, "no"])
misclass_rate_yes_simplified <- 
  conf_matrix_simplified["no", "yes"] / sum(conf_matrix_simplified[, "yes"])

# Calculate balanced accuracy
TPR_simplified <- 
  conf_matrix_simplified["yes", "yes"] / sum(conf_matrix_simplified[, "yes"])  
TNR_simplified <- 
  conf_matrix_simplified["no", "no"] / sum(conf_matrix_simplified[, "no"])
balanced_accuracy_simplified <- (TPR_simplified + TNR_simplified) / 2

# Print results
cat("Misclassification Rate for 'no' (simplified):", 
    misclass_rate_no_simplified, "\n")
cat("Misclassification Rate for 'yes' (simplified):", 
    misclass_rate_yes_simplified, "\n")
cat("Balanced Accuracy (simplified):", balanced_accuracy_simplified, "\n")
```

Yes, **stepwise selection leads to a slight improvement in balanced
accuracy**, increasing from **66.20% (weighted model)** to **66.31%
(simplified model)**. This improvement, while minor, indicates that
removing less impactful predictors helped simplify the model without
sacrificing performance.

-   Stepwise variable selection slightly improved balanced accuracy
    (from 66.20% to 66.31%), while keeping the misclassification rates
    for "yes" and "no" groups nearly the same.

-   The simplified model is more efficient, as it uses fewer predictors
    while maintaining similar performance to the weighted model.

# Task 1

## Load Data and Libraries

```{r}
# Load required libraries
library(ISLR)
library(glmnet)

# Load the dataset
data(Khan)

# Structure of the dataset
str(Khan)
```

The `Khan` dataset contains gene expression measurements for tissue
samples corresponding to four distinct types of small round blue cell
tumors. It includes. **Training Data (`xtrain`)**: 63 observations with
2308 gene expression measurements per sample. **Test Data (`xtest`)**:
20 observations with the same 2308 gene expression measurements.
**Response Variables**: `ytrain` and `ytest` represent the tumor types
for the training and test samples, respectively, encoded as numerical
labels (1, 2, 3, 4).

## (a) Why would LDA or QDA not work? Would RDA work?

### Why LDA or QDA Would Not Work:

1.  **High-Dimensional Data**:
    -   The dataset has 2308 predictors (genes) and only 63 training
        observations. Both **LDA (Linear Discriminant Analysis)** and
        **QDA (Quadratic Discriminant Analysis)** rely on estimating the
        covariance matrix of the predictors. However, the covariance
        matrix is $2308 \times 2308$, and its estimation requires more
        observations than predictors. With only 63 observations, the
        covariance matrix becomes singular (not invertible), making LDA
        and QDA inapplicable.
2.  **Overfitting Risk**:
    -   Even if these methods could handle the dimensionality, the small
        sample size relative to the large number of predictors would
        lead to severe overfitting, reducing their ability to generalize
        to new data.

### Would RDA Work?

**Regularized Discriminant Analysis (RDA)** introduces regularization to
address the overfitting problem by shrinking the covariance matrix.
Unlike LDA or QDA, RDA works by combining the identity matrix with the
covariance matrix, effectively reducing the complexity of the model.
using a tuning parameter to balance between LDA (shared covariance
matrix) and QDA (individual covariance matrices for each class).

Thus, **RDA could work here** because regularization mitigates the
issues of high dimensionality and small sample size. However, it might
still face challenges due to the extreme dimensionality of this dataset,
and methods like penalized regression (`glmnet`) or dimension reduction
may perform better.

A simpler identity matrix (which assumes all variables are independent and have the same variance)

## (b) Build a Model Using `cv.glmnet`

```{r build-model}
# Convert response to a factor
ytrain <- as.factor(Khan$ytrain)

# Fit multinomial model using cross-validation
fit_glmnet <- cv.glmnet(Khan$xtrain, ytrain, family = "multinomial")

# Plot cross-validation results
plot(fit_glmnet)

# Display the values of lambda.min and lambda.1se
cat("Lambda.min:", fit_glmnet$lambda.min, "\n")
cat("Lambda.1se:", fit_glmnet$lambda.1se, "\n")
```

1.  **Mean Multinomial Deviance**:
    -   The Y-axis represents the **multinomial deviance**, which
        measures how well the model fits the data. Lower values indicate
        better fit.
    -   The red dots are the mean deviance for each value of $\lambda$
        (the regularization parameter), and the vertical bars show the
        error around these estimates.
2.  **Log(**$\lambda$):
    -   The X-axis shows different values of the penalty parameter
        $\lambda$ (on a log scale). Larger $\lambda$ values correspond
        to more regularization (simpler models), while smaller $\lambda$
        values lead to less regularization (more complex models).
3.  **Two Vertical Lines**:
    -   The **left dashed line** represents `lambda.min`, the value of
        $\lambda$ that minimizes the mean deviance (best model fit).
    -   The **right dashed line** represents `lambda.1se`, the largest
        $\lambda$ value within one standard error of `lambda.min`. This
        gives a simpler model with slightly worse fit but better
        generalization.

### What is the Objective Function to Be Minimized?

The objective function being minimized is the **multinomial deviance**,
which measures how far off the model's predicted probabilities are from
the actual class labels.

Here’s what it means:

-   **If the model predicts probabilities close to the true class (e.g.,
    high for the correct class, low for others)**, the deviance is small
    (good).

-   **If the model predicts wrong probabilities**, the deviance is large
    (bad).

The formula is just a way of adding up how "wrong" the model’s guesses
are for all observations and all classes. The goal is to find the best
coefficients and $\lambda$ (penalty) that make the deviance as small as
possible, balancing good predictions with avoiding overfitting.

## (c) Which Variables Contribute to the Model?

```{r}
# Extract coefficients for all groups at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")

# Display coefficients for each group
for (group in names(coefficients)) {
  cat("\nCoefficients for group:", group, "\n")
  # print(as.matrix(coefficients[[group]]))
}

# Identify relevant coefficients (non-zero) for each group
relevant_coefficients <- lapply(coefficients, function(coef_matrix) {
  coef_matrix <- as.matrix(coef_matrix)
  relevant <- coef_matrix[coef_matrix != 0, , drop = FALSE]
  return(relevant)
})

# Print relevant coefficients
cat("\nRelevant coefficients for each group:\n")
print(relevant_coefficients)
```

**Which Variables Contribute to the Model?** Variables with non-zero
coefficients are those that contribute to the prediction of the response
variable. These variables are the ones that the model identifies as
significant predictors for the respective groups.

## (d) Plot a Relevant Variable Against the Response

```{r plot-variable}
selected_variable <- "V2022"

# Convert the training data to a data frame for ggplot2
training_data <- as.data.frame(Khan$xtrain)
training_data$response <- as.factor(Khan$ytrain)

# Generate the plot
library(ggplot2)

ggplot(training_data, aes_string(x = selected_variable, y = "response", 
                                 color = "response")) +
  geom_jitter(width = 0.2, height = 0.1, alpha = 0.6) +
  labs(
    title = paste("Distribution of", selected_variable, "by Response Group"),
    x = selected_variable,
    y = "Response (Groups)",
    color = "Group"
  ) +
  theme_minimal()
```

1.  **Group 1 (red points)**:
    -   The values of `V2022` for Group 1 are distinctively lower
        compared to other groups, clustering around values between -3
        and -2.
    -   This separation suggests that `V2022` is highly relevant for
        distinguishing Group 1 from the other groups.
2.  **Group 2 (green points)**:
    -   The values for Group 2 are mostly around -1 to 0, showing a
        different distribution compared to Group 1 and other groups.
3.  **Group 3 (blue points)**:
    -   The values of `V2022` for Group 3 are slightly above 0, with a
        tighter clustering compared to Groups 1 and 2.
4.  **Group 4 (purple points)**:
    -   The values of `V2022` for Group 4 are the highest, with a clear
        separation from all other groups. This suggests that `V2022`
        might also contribute to distinguishing Group 4.

**`V2022` is highly relevant for Group 1** as its values for this group
are uniquely lower compared to other groups. The variable shows clear
separations between groups, which indicates its importance in predicting
the group labels. The distinct clustering of values for each group
suggests that `V2022` could be a strong predictive feature for this
classification problem.

## (e) Predict and Evaluate on Test Data

```{r evaluate-model}
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, 
                           s = "lambda.1se", type = "response")

# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)

# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)

# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)

# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
```

The model has perfectly classified all the test samples into their
correct groups. This is likely due to the high dimensionality of the
dataset and the distinct separation of the classes in the predictor
space (as seen in the variable V2022).
