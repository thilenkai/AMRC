# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
if (!requireNamespace("pls", quietly = TRUE)) {
install.packages("pls")
}
library(pls)
library(cvTools)
library(leaps)
# Load the dataset
load("building.RData")
#str(df)
set.seed(12332281)
# Split the data
sample_index <- sample(1:nrow(df), size = 2/3 * nrow(df))
train_data <- df[sample_index, ]
test_data <- df[-sample_index, ]
# Display the number of samples in each set
cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
# Fit linear regression model
model <- lm(y ~ ., data = train_data)
# summary(model)
train_pred <- predict(model, newdata = train_data)
# Compute RMSE for training data
train_rmse <- sqrt(mean((train_data$y - train_pred)^2))
cat("Training RMSE:", train_rmse, "\n")
# Load the dataset
load("building.RData")
str(df)
if (!requireNamespace("glmnet", quietly = TRUE)) {
install.packages("glmnet")
}
# Load the necessary libraries
library(glmnet)
# Load the dataset
load("building.RData")
str(df)
set.seed(12332281)
# Split the data
sample_index <- sample(1:nrow(df), size = 2/3 * nrow(df))
train_data <- df[sample_index, ]
test_data <- df[-sample_index, ]
# Display the number of samples in each set
cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
# Fit linear regression model
model <- lm(y ~ ., data = train_data)
# summary(model)
train_pred <- predict(model, newdata = train_data)
# Compute RMSE for training data
train_rmse <- sqrt(mean((train_data$y - train_pred)^2))
cat("Training RMSE:", train_rmse, "\n")
# Prepare the predictor and response matrices for the training data
x_train <- as.matrix(train_data[, -1])  # Exclude response variable
y_train <- train_data$y
# Fit Ridge Regression model
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Plot the ridge model to visualize the coefficient paths
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Coefficient Paths")
# Fit Ridge Regression model
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Plot the ridge model to visualize the coefficient paths
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Coefficient Paths")
# Fit Ridge Regression model
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Plot the ridge model to visualize the coefficient paths
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Coefficient Paths")
# Fit Ridge Regression model
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Plot the ridge model to visualize the coefficient paths
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Coefficient Paths")
# Fit Ridge Regression model
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Plot the ridge model to visualize the coefficient paths
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Coefficient Paths")
# Perform cross-validation to determine the best lambda
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0)
plot(ridge_cv, main = "Cross-Validation for Ridge Regression")
# Best lambda (1 standard error rule)
optimal_lambda <- ridge_cv$lambda.1se
cat("Optimal lambda (1-SE rule):", optimal_lambda, "\n")
# Prepare the predictor and response matrices for the training data
x_train <- as.matrix(train_data[, -1])  # Exclude response variable
y_train <- train_data$y
# Fit Ridge Regression model
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Plot the ridge model to visualize the coefficient paths
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Coefficient Paths")
# Prepare the predictor and response matrices for the training data
x_train <- as.matrix(train_data[, -1])  # Assuming the first column is the response variable 'y'
y_train <- train_data$y
# Perform cross-validation with Ridge Regression (alpha = 0)
set.seed(123)  # For reproducibility
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0)
# Plot the cross-validation results
plot(ridge_cv, main = "Cross-Validation for Ridge Regression")
# Print the optimal lambda values
cat("Lambda with minimum MSE:", ridge_cv$lambda.min, "\n")
cat("Lambda with 1-SE rule:", ridge_cv$lambda.1se, "\n")
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
if (!requireNamespace("glmnet", quietly = TRUE)) {
install.packages("glmnet")
}
# Load the necessary libraries
library(glmnet)
# Load the dataset
load("building.RData")
# str(df)
# Fit linear regression model
model <- lm(y ~ ., data = train_data)
# summary(model)
train_pred <- predict(model, newdata = train_data)
# Compute RMSE for training data
train_rmse <- sqrt(mean((train_data$y - train_pred)^2))
cat("Training RMSE:", train_rmse, "\n")
# Prepare the predictor and response matrices for the training data
x_train <- as.matrix(train_data[, -1])  # Exclude response variable
y_train <- train_data$y
# Fit Ridge Regression model
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Plot the ridge model to visualize the coefficient paths
plot(ridge_model, xvar = "lambda", label = TRUE, main = "Ridge Regression Coefficient Paths")
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0)
# Plot the cross-validation results
plot(ridge_cv, main = "Cross-Validation for Ridge Regression")
# Print the optimal lambda values
cat("Lambda with minimum MSE:", ridge_cv$lambda.min, "\n")
cat("Lambda with 1-SE rule:", ridge_cv$lambda.1se, "\n")
# Optimal lambda values
optimal_lambda_min <- ridge_cv$lambda.min
optimal_lambda_1se <- ridge_cv$lambda.1se
# Display optimal lambda values
cat("Lambda with minimum MSE:", optimal_lambda_min, "\n")
cat("Lambda with 1-SE rule:", optimal_lambda_1se, "\n")
# Obtain coefficients for the model using lambda.min
coef_min <- coef(ridge_cv, s = "lambda.min")
cat("Coefficients with lambda.min:\n")
print(coef_min)
# Obtain coefficients for the model using lambda.1se
coef_1se <- coef(ridge_cv, s = "lambda.1se")
cat("Coefficients with lambda.1se:\n")
print(coef_1se)
# Optimal lambda values
optimal_lambda_min <- ridge_cv$lambda.min
optimal_lambda_1se <- ridge_cv$lambda.1se
# Display optimal lambda values
cat("Lambda with minimum MSE:", optimal_lambda_min, "\n")
cat("Lambda with 1-SE rule:", optimal_lambda_1se, "\n")
# Obtain coefficients for the model using lambda.min
coef_min <- coef(ridge_cv, s = "lambda.min")
cat("Coefficients with lambda.min:\n")
print(coef_min)
# Obtain coefficients for the model using lambda.1se
coef_1se <- coef(ridge_cv, s = "lambda.1se")
cat("Coefficients with lambda.1se:\n")
print(coef_1se)
# Convert the sparse matrix to a data frame
coef_min_df <- as.data.frame(as.matrix(coef_min))
coef_1se_df <- as.data.frame(as.matrix(coef_1se))
# Display the data frames
print(coef_min_df)
print(coef_1se_df)
# Obtain coefficients for the model using lambda.min
coef_min <- coef(ridge_cv, s = "lambda.min")
# Obtain coefficients for the model using lambda.1se
coef_1se <- coef(ridge_cv, s = "lambda.1se")
# Convert the sparse matrix to a data frame
coef_min_df <- as.data.frame(as.matrix(coef_min))
coef_1se_df <- as.data.frame(as.matrix(coef_1se))
print(coef_min_df)
print(coef_1se_df)
# Obtain coefficients for the model using lambda.min
coef_min <- coef(ridge_cv, s = "lambda.min")
# Obtain coefficients for the model using lambda.1se
coef_1se <- coef(ridge_cv, s = "lambda.1se")
# Convert the sparse matrix to a data frame
coef_min_df <- as.data.frame(as.matrix(coef_min))
coef_1se_df <- as.data.frame(as.matrix(coef_1se))
print(coef_min_df)
print(coef_1se_df)
# Prepare test data matrices
x_test <- as.matrix(test_data[, -1])  # Exclude response variable from predictors
y_test <- test_data$y
# Predict using lambda.min
pred_min <- predict(ridge_cv, s = "lambda.min", newx = x_test)
# Predict using lambda.1se
pred_1se <- predict(ridge_cv, s = "lambda.1se", newx = x_test)
# RMSE for lambda.min
rmse_min <- sqrt(mean((y_test - pred_min)^2))
cat("RMSE with lambda.min:", rmse_min, "\n")
# RMSE for lambda.1se
rmse_1se <- sqrt(mean((y_test - pred_1se)^2))
cat("RMSE with lambda.1se:", rmse_1se, "\n")
plot(y_test, pred_min,
main = "Predictions vs. Actual Values (lambda.min)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "blue")
abline(0, 1, col = "red", lwd = 2)  # Add 45-degree line for reference
plot(y_test, pred_min,
main = "Predictions vs. Actual Values (lambda.min)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)  # Add 45-degree line for reference
plot(y_test, pred_1se,
main = "Predictions vs. Actual Values (lambda.1se)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightgreen")
abline(0, 1, col = "red", lwd = 2)  # Add 45-degree line for reference
# Prepare test data matrices
x_test <- as.matrix(test_data[, -1])
y_test <- test_data$y
# Predict using lambda.min - the optimal model
pred_min <- predict(ridge_cv, s = "lambda.min", newx = x_test)
# Predict using lambda.1se
# pred_1se <- predict(ridge_cv, s = "lambda.1se", newx = x_test)
# RMSE for lambda.min
rmse_min <- sqrt(mean((y_test - pred_min)^2))
cat("RMSE with lambda.min:", rmse_min, "\n")
# RMSE for lambda.1se
# rmse_1se <- sqrt(mean((y_test - pred_1se)^2))
# cat("RMSE with lambda.1se:", rmse_1se, "\n")
plot(y_test, pred_min,
main = "Predictions vs. Actual Values (lambda.min)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)
# plot(y_test, pred_1se,
#      main = "Predictions vs. Actual Values (lambda.1se)",
#      xlab = "Actual Values",
#      ylab = "Predicted Values",
#      pch = 16, col = "lightgreen")
# abline(0, 1, col = "red", lwd = 2)
# Fit Lasso Regression model (alpha = 1 by default for Lasso)
lasso_model <- glmnet(x_train, y_train, alpha = 1)
# Plot Lasso model to visualize coefficient paths
plot(lasso_model, xvar = "lambda", main = "Lasso Regression Coefficient Paths")
# Perform cross-validation to find optimal lambda for Lasso
set.seed(123)  # For reproducibility
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)
# Plot cross-validated MSE for different lambda values
plot(lasso_cv, main = "Cross-Validation for Lasso Regression")
# Extract coefficients for lambda.min and lambda.1se
coef_lasso_min <- coef(lasso_cv, s = "lambda.min")
coef_lasso_1se <- coef(lasso_cv, s = "lambda.1se")
# Convert sparse matrices to data frames for easier viewing
coef_lasso_min_df <- as.data.frame(as.matrix(coef_lasso_min))
coef_lasso_1se_df <- as.data.frame(as.matrix(coef_lasso_1se))
# Display coefficients
print(coef_lasso_min_df)
print(coef_lasso_1se_df)
# Predict using lambda.min and lambda.1se
pred_lasso_min <- predict(lasso_cv, s = "lambda.min", newx = x_test)
pred_lasso_1se <- predict(lasso_cv, s = "lambda.1se", newx = x_test)
# Calculate RMSE for both lambda.min and lambda.1se
rmse_lasso_min <- sqrt(mean((y_test - pred_lasso_min)^2))
rmse_lasso_1se <- sqrt(mean((y_test - pred_lasso_1se)^2))
cat("RMSE with lambda.min:", rmse_lasso_min, "\n")
cat("RMSE with lambda.1se:", rmse_lasso_1se, "\n")
plot(y_test, pred_lasso_min,
main = "Predictions vs. Actual Values (Lasso, lambda.min)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)
plot(y_test, pred_lasso_1se,
main = "Predictions vs. Actual Values (Lasso, lambda.1se)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightgreen")
abline(0, 1, col = "red", lwd = 2)
# Fit Lasso Regression model (alpha = 1 by default for Lasso)
lasso_model <- glmnet(x_train, y_train, alpha = 1)
# Plot Lasso model to visualize coefficient paths
plot(lasso_model, xvar = "lambda", main = "Lasso Regression Coefficient Paths")
# Perform cross-validation with Ridge Regression (alpha = 0)
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0)
# Plot the cross-validation results
plot(ridge_cv, main = "Cross-Validation for Ridge Regression")
# Print the optimal lambda values
cat("Lambda with minimum MSE:", ridge_cv$lambda.min, "\n")
cat("Lambda with 1-SE rule:", ridge_cv$lambda.1se, "\n")
# Perform cross-validation to find optimal lambda for Lasso
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)
# Plot cross-validated MSE for different lambda values
plot(lasso_cv, main = "Cross-Validation for Lasso Regression")
# Print the optimal lambda values
cat("Lambda with minimum MSE:", lasso_cv$lambda.min, "\n")
cat("Lambda with 1-SE rule:", lasso_cv$lambda.1se, "\n")
# Extract coefficients for lambda.min and lambda.1se
coef_lasso_min <- coef(lasso_cv, s = "lambda.min")
coef_lasso_1se <- coef(lasso_cv, s = "lambda.1se")
# Convert sparse matrices to data frames for easier viewing
coef_lasso_min_df <- as.data.frame(as.matrix(coef_lasso_min))
coef_lasso_1se_df <- as.data.frame(as.matrix(coef_lasso_1se))
# Display coefficients
print(coef_lasso_min_df)
print(coef_lasso_1se_df)
# Predict using lambda.min and lambda.1se
pred_lasso_min <- predict(lasso_cv, s = "lambda.min", newx = x_test)
pred_lasso_1se <- predict(lasso_cv, s = "lambda.1se", newx = x_test)
# Calculate RMSE for both lambda.min and lambda.1se
rmse_lasso_min <- sqrt(mean((y_test - pred_lasso_min)^2))
rmse_lasso_1se <- sqrt(mean((y_test - pred_lasso_1se)^2))
cat("RMSE with lambda.min:", rmse_lasso_min, "\n")
cat("RMSE with lambda.1se:", rmse_lasso_1se, "\n")
plot(y_test, pred_lasso_min,
main = "Predictions vs. Actual Values (Lasso, lambda.min)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)
plot(y_test, pred_lasso_min,
main = "Predictions vs. Actual Values (Lasso, lambda.min)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)
plot(y_test, pred_lasso_1se,
main = "Predictions vs. Actual Values (Lasso, lambda.1se)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightgreen")
abline(0, 1, col = "red", lwd = 2)
# Predict using lambda.min and lambda.1se
# pred_lasso_min <- predict(lasso_cv, s = "lambda.min", newx = x_test)
# pred_lasso_1se <- predict(lasso_cv, s = "lambda.1se", newx = x_test)
# Calculate RMSE for both lambda.min and lambda.1se
rmse_lasso_min <- sqrt(mean((y_test - pred_lasso_min)^2))
rmse_lasso_1se <- sqrt(mean((y_test - pred_lasso_1se)^2))
# cat("RMSE with lambda.min:", rmse_lasso_min, "\n")
cat("RMSE with lambda.1se:", rmse_lasso_1se, "\n")
# Fit Ridge Regression model (alpha = 0)
ridge_model <- glmnet(x_train, y_train, alpha = 0)
# Extract Ridge coefficients for a selected lambda (e.g., lambda.min from cross-validation)
ridge_cv <- cv.glmnet(x_train, y_train, alpha = 0)
ridge_coef <- coef(ridge_cv, s = "lambda.min")
# Compute weights for Adaptive Lasso (inverse absolute Ridge coefficients)
# Exclude intercept by using [-1] to avoid division by zero
ridge_weights <- 1 / abs(ridge_coef[-1])
ridge_weights[is.infinite(ridge_weights)] <- 0  # Set infinite weights to zero for stability
# Fit Adaptive Lasso model using the Ridge weights
adaptive_lasso_model <- glmnet(x_train, y_train, alpha = 1, penalty.factor = ridge_weights)
# Plot the Adaptive Lasso model to visualize coefficient paths
plot(adaptive_lasso_model, xvar = "lambda", main = "Adaptive Lasso Regression Coefficient Paths")
# Perform cross-validation for Adaptive Lasso
adaptive_lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1, penalty.factor = ridge_weights)
# Plot cross-validation results
plot(adaptive_lasso_cv, main = "Cross-Validation for Adaptive Lasso Regression")
# Obtain coefficients for lambda.min and lambda.1se
coef_adaptive_lasso_min <- coef(adaptive_lasso_cv, s = "lambda.min")
coef_adaptive_lasso_1se <- coef(adaptive_lasso_cv, s = "lambda.1se")
# Convert sparse matrices to data frames for easier viewing
coef_adaptive_lasso_min_df <- as.data.frame(as.matrix(coef_adaptive_lasso_min))
coef_adaptive_lasso_1se_df <- as.data.frame(as.matrix(coef_adaptive_lasso_1se))
# Display coefficients
print(coef_adaptive_lasso_min_df)
print(coef_adaptive_lasso_1se_df)
# Predict using lambda.min and lambda.1se
pred_adaptive_lasso_min <- predict(adaptive_lasso_cv, s = "lambda.min", newx = x_test)
pred_adaptive_lasso_1se <- predict(adaptive_lasso_cv, s = "lambda.1se", newx = x_test)
# Calculate RMSE for both lambda.min and lambda.1se
rmse_adaptive_lasso_min <- sqrt(mean((y_test - pred_adaptive_lasso_min)^2))
rmse_adaptive_lasso_1se <- sqrt(mean((y_test - pred_adaptive_lasso_1se)^2))
cat("RMSE with lambda.min:", rmse_adaptive_lasso_min, "\n")
cat("RMSE with lambda.1se:", rmse_adaptive_lasso_1se, "\n")
plot(y_test, pred_adaptive_lasso_min,
main = "Predictions vs. Actual Values (Adaptive Lasso, lambda.min)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)
plot(y_test, pred_adaptive_lasso_1se,
main = "Predictions vs. Actual Values (Adaptive Lasso, lambda.1se)",
xlab = "Actual Values",
ylab = "Predicted Values",
pch = 16, col = "lightgreen")
abline(0, 1, col = "red", lwd = 2)
