---
title: "Exercise 3"
subtitle: "Advanced Methods for Regression and Classification"
author: "Rita Selimi"
date: "11/05/2024"
output: 
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
```

## Load the necessary libraries

```{r, include=FALSE}
if (!requireNamespace("pls", quietly = TRUE)) {
  install.packages("pls")
}

library(pls)
library(cvTools)
library(leaps)
```

## Load Data

```{r load-data}
# Load the dataset
load("building.RData")
#str(df)
```

## Data Splitting

```{r data-splitting}
set.seed(12332281)

# Split the data
sample_index <- sample(1:nrow(df), size = 2/3 * nrow(df))
train_data <- df[sample_index, ]
test_data <- df[-sample_index, ]

# Display the number of samples in each set
cat("Training set size:", nrow(train_data), "\n")
cat("Test set size:", nrow(test_data), "\n")
```
### Fit regression model on the training data

```{r model-fitting}
# Fit linear regression model
model <- lm(y ~ ., data = train_data)
# summary(model)
```

### Compute RMSE (training data)

```{r predictions}
train_pred <- predict(model, newdata = train_data)

# Compute RMSE for training data
train_rmse <- sqrt(mean((train_data$y - train_pred)^2))
cat("Training RMSE:", train_rmse, "\n")
```

## 1. Principal Component Regression (PCR) 

### a) Applying PCR with Cross-Validation

```{r pcr-model}
# Fit PCR model with cross-validation and scaling
pcr_model <- pcr(y ~ ., data = train_data, scale = TRUE, 
                 validation = "CV", segments = 10)
summary(pcr_model)
```
This applies Principal Component Regression (PCR) to the training data, scaling the variables and using 10-fold cross-validation to find the best number of components for accurate predictions.

### b) Cross-Validation Error Plot and Optimal Components

```{r cv-error-plot}
# Plot cross-validation errors for each component
validationplot(pcr_model, val.type = "MSEP", 
               main = "PCR Cross-Validation Error Plot")
```
**How many components seem to be optimal?**

Optimal number of components is around 68.
The curve shows how the prediction error changes as more components are added. Initially, the error decreases, indicating an improvement in the model's accuracy as components are included. However, after a certain number of components, the error increases, suggesting that adding more components leads to overfitting, where the model becomes too complex and loses its generalizability. 

```{r}
rmsep_results <- RMSEP(pcr_model)
rmsep_values <- rmsep_results$val

# Now you can find the optimal RMSE
optimal_components <- 35
optimal_rmse <- rmsep_values[optimal_components]
cat("Resulting RMSE at optimal components:", optimal_rmse, "\n")
```
The RMSE increased when moving from a simple training RMSE to a cross-validated RMSE in PCR, bacause:
- Overfitting on Training Data: The initial RMSE of 0.1929 was calculated directly on the training data, which means the model was fitted to the same data used to evaluate it. 
- Cross-Validation for Generalization: When you switch to using cross-validation in PCR, the model is repeatedly trained on different subsets of the data and tested on other subsets. The cross-validated RMSE, better reflects how well the model generalizes to unseen data. 

### c) Plotting Measured vs. Cross-Validated Predictions

```{r predplot}
# Plot measured vs cross-validated predictions
predplot(pcr_model, ncomp = optimal_components, 
         main = "Measured vs Cross-Validated Predictions")
```
This plot visualizes the relationship between the actual measured values of the response variable (`y`) and the cross-validated predictions generated by the PCR model using 68 components. The plot shows that the cross-validated predictions closely align with the actual values along the diagonal, indicating the model’s generally accurate fit, with some minor variance at the edges. 

### d) Plotting Predicted vs Observed Values for Test Data and Computing RMSE
```{r}
test_pred <- predict(pcr_model, newdata = test_data, ncomp = optimal_components)

# Plot Predicted vs Observed values for Test Data
plot(test_data$y, test_pred, 
     main = "Predicted vs Actual Response (Test Data)", 
     xlab = "Actual Response", 
     ylab = "Predicted Values", 
     pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)

# Calculate RMSE for Test Data
test_rmse <- sqrt(mean((test_data$y - test_pred)^2))
cat("Test RMSE:", test_rmse, "\n")
```
The plot shows a strong alignment of the predicted values with the actual values for the test data. The relatively low Test RMSE of 0.2872, indicates minimal error in predictions.

## 2. Partial least squares regression (PLS)

### a) Applying PLS with Cross-Validation

```{r pls-model}
# Apply PLS on the training data with cross-validation and scaling
pls_model <- plsr(y ~ ., data = train_data, scale = TRUE, 
                  validation = "CV", segments = 10)
summary(pls_model)
```
This code fits a PLS model to the train_data dataset, using scaling and 10-fold cross-validation to assess model performance.

### b) Cross-Validation Error Plot and Optimal Components

```{r pls-error-plot}
# Plot cross-validation errors for each component
validationplot(pls_model, val.type = "MSEP", 
               main = "PLS Cross-Validation Error Plot")
```
**How many components seem to be optimal?**
In the PLS Cross-Validation Error Plot, the optimal number of components appears to be where the Mean Squared Error of Prediction (MSEP) is at its lowest before any significant increase. From the plot, this seems to occur at around 67 components. After this point, the MSEP value increases sharply, indicating that adding more components does not improve the model's performance and may lead to overfitting.

```{r}
rmsep_results2 <- RMSEP(pls_model)
rmsep_values2 <- rmsep_results2$val

# Now you can find the optimal RMSE
optimal_components2 <- 19
optimal_rmse <- rmsep_values2[optimal_components]
cat("Resulting RMSE at optimal components:", optimal_rmse, "\n")
```

### c) Plotting Measured vs. Cross-Validated Predictions

```{r pls-predplot}
# Plot measured vs cross-validated predictions
predplot(pls_model, ncomp = optimal_components2, 
         main = "Measured vs Cross-Validated Predictions")
```
This plot visualizes the relationship between the actual measured values of the response variable (`y`) and the cross-validated predictions generated by the PLS model using 67 components. The plot shows that the cross-validated predictions closely align with the actual values along the diagonal, indicating the model’s generally accurate fit, with some minor variance at the edges. 

### d) Plotting Predicted vs Observed Values for Test Data and Computing RMSE
```{r}
pls_test_pred <- predict(pls_model, newdata = test_data, ncomp = optimal_components2)

# Plot Predicted vs Observed values for Test Data
plot(test_data$y, test_pred, 
     main = "Predicted vs Actual Response (Test Data)", 
     xlab = "Actual Response", 
     ylab = "Predicted Values", 
     pch = 16, col = "lightblue")
abline(0, 1, col = "red", lwd = 2)  

# Calculate RMSE for test data predictions
pls_test_rmse <- sqrt(mean((test_data$y - pls_test_pred)^2))
cat("Test RMSE for PLS:", pls_test_rmse, "\n")
```
The plot shows a strong alignment of the predicted values with the actual values for the test data. The relatively low Test RMSE of 0.2992037, indicates minimal error in predictions.

### e) ) Compare the regression coefficients from PCR and PLS
```{r}
# Extract coefficients for the optimal number of components in PCR and PLS
pcr_coefs <- coef(pcr_model, ncomp = optimal_components)
pls_coefs <- coef(pls_model, ncomp = optimal_components2)

# Convert to data frame for easy plotting
coef_data <- data.frame(
  Predictor = rownames(pcr_coefs),
  PCR = as.vector(pcr_coefs),
  PLS = as.vector(pls_coefs)
)

# Plot the coefficients for PCR and PLS
library(ggplot2)
ggplot(coef_data, aes(x = Predictor)) +
  geom_bar(aes(y = PCR, fill = "PCR"), stat = "identity", position = "dodge") +
  geom_bar(aes(y = PLS, fill = "PLS"), stat = "identity", position = "dodge") +
  labs(title = "Comparison of Regression Coefficients for PCR and PLS",
       y = "Coefficient Value") +
  scale_fill_manual(values = c("PCR" = "blue", "PLS" = "lightgreen")) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme_minimal()
```
Direction: 
Positive values indicate that an increase in the predictor value is associated with an increase in the response.
Negative values indicate that an increase in the predictor value is associated with a decrease in the response.

From this comparison plot of regression coefficients for PCR (blue) and PLS (green), we can observe:

1. **Coefficient Magnitudes**: PLS generally shows larger coefficients than PCR across several predictors, suggesting that PLS places more emphasis on certain variables than PCR. This is because PLS aims to maximize the covariance between predictors and the response variable, while PCR focuses on capturing the variance of predictors.

2. **Direction of Coefficients**: In some predictors, the direction of the coefficients differs between PCR and PLS, indicating that each method might be capturing slightly different relationships between predictors and the response.

3. **Stability and Selection**: PCR seems to yield smaller coefficients, particularly around zero for several predictors, which could indicate that only the most influential predictors have noticeable effects. 

## 3. Score vectors and loadings vectors

```{r}
library(ggplot2)
library(gridExtra)

# Extract scores and loadings for PCR
pcr_scores <- pcr_model$scores[, 1:2]  # First two score vectors (Z1 and Z2)
pcr_loadings <- pcr_model$loadings[, 1:2]  # First two loading vectors (V1 and V2)

# Extract scores and loadings for PLS
pls_scores <- pls_model$scores[, 1:2]  # First two score vectors (T1 and T2)
pls_loadings <- pls_model$loadings[, 1:2]  # First two loading vectors (W1 and W2)

# Convert to data frames for ggplot
pcr_scores_df <- as.data.frame(pcr_scores)
pcr_loadings_df <- as.data.frame(pcr_loadings)
pls_scores_df <- as.data.frame(pls_scores)
pls_loadings_df <- as.data.frame(pls_loadings)

# Plot PCR scores
p1 <- ggplot(pcr_scores_df, aes(x = `Comp 1`, y = `Comp 2`)) +
  geom_point(color = "blue") +
  labs(title = "PCR Scores (Z1 vs Z2)", x = "Z1", y = "Z2")

# Plot PCR loadings
p2 <- ggplot(pcr_loadings_df, aes(x = `Comp 1`, y = `Comp 2`)) +
  geom_point(color = "red") +
  labs(title = "PCR Loadings (V1 vs V2)", x = "V1", y = "V2")

# Plot PLS scores
p3 <- ggplot(pls_scores_df, aes(x = `Comp 1`, y = `Comp 2`)) +
  geom_point(color = "green") +
  labs(title = "PLS Scores (T1 vs T2)", x = "T1", y = "T2")

# Plot PLS loadings
p4 <- ggplot(pls_loadings_df, aes(x = `Comp 1`, y = `Comp 2`)) +
  geom_point(color = "purple") +
  labs(title = "PLS Loadings (W1 vs W2)", x = "W1", y = "W2")

# Arrange the plots in a 2x2 grid
grid.arrange(p1, p2, p3, p4, ncol = 2)
```
Scores, represents the transformed variables (principal components) derived from the original predictors.
Loadings, represents how much each original predictor contributes to each principal component.

1. **PCR Scores (Z1 vs Z2)**: The plot of the first two principal component scores (Z1 and Z2) shows a clear pattern, indicating that the first two components capture a meaningful structure in the data. There seems to be a trend within the points, suggesting a relationship between the variables that these components represent.

2. **PCR Loadings (V1 vs V2)**: This plot of the loadings (V1 and V2) shows how the original predictors contribute to the first two principal components. The spread of points here implies that certain variables are more heavily weighted on either V1 or V2, helping shape the data's primary directions.

3. **PLS Scores (T1 vs T2)**: The PLS scores (T1 and T2) also show a pattern, with points spread across a curve. This indicates that PLS, like PCR, captures a trend in the data. However, PLS components focus on maximizing the correlation with the response variable, so these patterns are influenced by that goal.

4. **PLS Loadings (W1 vs W2)**: The loadings for PLS (W1 vs W2) indicate how the original variables contribute to the first two PLS components. The distribution of points suggests some variables contribute more to these components, capturing relationships specifically targeted to predict the response.
