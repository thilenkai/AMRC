y = "Response Group",
color = "Group"
) +
theme_minimal() +
theme(
legend.position = "bottom",
plot.title = element_text(hjust = 0.5, size = 16),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)
)
# Assuming the training data is in a dataframe `train_data`
# Replace `response_column` with the name of your response column
# Replace `variable_name` with the name of the selected variable (e.g., "V1")
# Example:
response_column <- "Group"
variable_name <- "V123"
# Load necessary library
library(ggplot2)
# Generate the plot
ggplot(train_data, aes(x = .data[[variable_name]], y = .data[[response_column]], color = as.factor(.data[[response_column]]))) +
geom_point(alpha = 0.7, size = 3) +
labs(
title = paste("Scatter Plot of", variable_name, "vs Response"),
x = variable_name,
y = "Response Group",
color = "Group"
) +
theme_minimal() +
theme(
legend.position = "bottom",
plot.title = element_text(hjust = 0.5, size = 16),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)
)
# Extract coefficients for the model at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
cat("Coefficients for each group:\n")
print(coefficients)
# Extract coefficients for the model at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
cat("Coefficients for each group:\n")
#print(coefficients)
# Assuming the training data is in a dataframe `train_data`
# Replace `response_column` with the name of your response column
# Replace `variable_name` with the name of the selected variable (e.g., "V1")
# Example:
response_column <- "Group"
variable_name <- "V1"
# Load necessary library
library(ggplot2)
# Generate the plot
ggplot(train_data, aes(x = .data[[variable_name]], y = .data[[response_column]], color = as.factor(.data[[response_column]]))) +
geom_point(alpha = 0.7, size = 3) +
labs(
title = paste("Scatter Plot of", variable_name, "vs Response"),
x = variable_name,
y = "Response Group",
color = "Group"
) +
theme_minimal() +
theme(
legend.position = "bottom",
plot.title = element_text(hjust = 0.5, size = 16),
axis.text = element_text(size = 12),
axis.title = element_text(size = 14)
)
# Extract coefficients for the model at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
cat("Coefficients for each group:\n")
print(coefficients)
# Extract coefficients for the model at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
cat("Coefficients for each group:\n")
# print(coefficients)
# Extract coefficients for the model at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
cat("Coefficients for each group:\n")
# print(coefficients)
# Extract coefficients for all groups at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
for (group in names(coefficients)) {
cat("\nCoefficients for group:", group, "\n")
print(as.matrix(coefficients[[group]]))
}
# Identify relevant coefficients (non-zero) for each group
relevant_coefficients <- lapply(coefficients, function(coef_matrix) {
coef_matrix <- as.matrix(coef_matrix)
relevant <- coef_matrix[coef_matrix != 0, , drop = FALSE]
return(relevant)
})
# Print relevant coefficients
cat("\nRelevant coefficients for each group:\n")
print(relevant_coefficients)
# Extract coefficients for all groups at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
for (group in names(coefficients)) {
cat("\nCoefficients for group:", group, "\n")
# print(as.matrix(coefficients[[group]]))
}
# Identify relevant coefficients (non-zero) for each group
relevant_coefficients <- lapply(coefficients, function(coef_matrix) {
coef_matrix <- as.matrix(coef_matrix)
relevant <- coef_matrix[coef_matrix != 0, , drop = FALSE]
return(relevant)
})
# Print relevant coefficients
cat("\nRelevant coefficients for each group:\n")
print(relevant_coefficients)
# Assuming the training data is stored in a data frame named `training_data`
# with the response variable as `response` and predictors including `V2022`
# Select relevant variable for Group 1
selected_variable <- "V2022"
# Generate the plot
library(ggplot2)
ggplot(training_data, aes_string(x = selected_variable, y = "response", color = "response")) +
geom_jitter(width = 0.2, height = 0.2, alpha = 0.6) +
scale_color_manual(values = c("Group 1" = "blue", "Other Groups" = "red")) +
labs(
title = paste("Distribution of", selected_variable, "by Response"),
x = selected_variable,
y = "Response (Groups)",
color = "Group"
) +
theme_minimal()
# Ensure the variable "V2022" is included in the dataset and extract it
selected_variable <- "V2022"  # Replace with any other relevant variable if needed
# Convert the training data to a data frame for ggplot2
training_data <- as.data.frame(Khan$xtrain)
training_data$response <- as.factor(Khan$ytrain)  # Add response as a factor
# Generate the plot
library(ggplot2)
ggplot(training_data, aes_string(x = selected_variable, y = "response", color = "response")) +
geom_jitter(width = 0.2, height = 0.1, alpha = 0.6) +
labs(
title = paste("Distribution of", selected_variable, "by Response Group"),
x = selected_variable,
y = "Response (Groups)",
color = "Group"
) +
theme_minimal()
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, s = "lambda.1se", type = "response")
# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)
# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)
# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)
# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, s = "lambda.1se", type = "response")
# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)
# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)
# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)
# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, s = "lambda.min", type = "response")
# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)
# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)
# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)
# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, s = "lambda.1se", type = "response")
# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)
# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)
# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)
# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, s = "lambda.1se", type = "response")
# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)
# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)
# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)
# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
# Load the data
d <- read.csv2("bank.csv")
# Exclude the variable 'duration'
d <- d[, !colnames(d) %in% "duration"]
# Convert target variable to a factor
d$y <- as.factor(d$y)
# Check class distribution
table(d$y)
head(d)
summary(d)
set.seed(123)
# Randomly select 3000 observations for training
train_indices <- sample(nrow(d), 3000)
train_data <- d[train_indices, ]
test_data <- d[-train_indices, ]
# Fit logistic regression model
logit_model <- glm(y ~ ., data = train_data, family = "binomial")
# Summary of the model
summary(logit_model)
# Predict probabilities for the test set
test_data$predicted_prob <- predict(logit_model, newdata = test_data, type = "response")
# Convert probabilities to class labels (default threshold = 0.5)
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, "yes", "no")
# Create a confusion matrix
conf_matrix <- table(Predicted = test_data$predicted_class, Actual = test_data$y)
# Calculate misclassification rate for each group
misclass_rate_no <- conf_matrix["yes", "no"] / sum(conf_matrix[, "no"])
misclass_rate_yes <- conf_matrix["no", "yes"] / sum(conf_matrix[, "yes"])
# Calculate balanced accuracy
TPR <- conf_matrix["yes", "yes"] / sum(conf_matrix[, "yes"])  # True Positive Rate
TNR <- conf_matrix["no", "no"] / sum(conf_matrix[, "no"])     # True Negative Rate
balanced_accuracy <- (TPR + TNR) / 2
# Print results
cat("Misclassification Rate for 'no':", misclass_rate_no, "\n")
cat("Misclassification Rate for 'yes':", misclass_rate_yes, "\n")
cat("Balanced Accuracy:", balanced_accuracy, "\n")
# Calculate weights
class_weights <- ifelse(train_data$y == "yes",
1 / sum(train_data$y == "yes"),
1 / sum(train_data$y == "no"))
# Normalize the weights
class_weights <- class_weights * length(train_data$y)
# Fit weighted logistic regression model
weighted_logit_model <- glm(y ~ ., data = train_data, family = "binomial", weights = class_weights)
# Predict probabilities on the test set
test_data$weighted_prob <- predict(weighted_logit_model, newdata = test_data, type = "response")
# Convert probabilities to class labels (default threshold = 0.5)
test_data$weighted_class <- ifelse(test_data$weighted_prob > 0.5, "yes", "no")
# Create a confusion matrix
conf_matrix_weighted <- table(Predicted = test_data$weighted_class, Actual = test_data$y)
# Calculate misclassification rate for each group
misclass_rate_no_weighted <- conf_matrix_weighted["yes", "no"] / sum(conf_matrix_weighted[, "no"])
misclass_rate_yes_weighted <- conf_matrix_weighted["no", "yes"] / sum(conf_matrix_weighted[, "yes"])
# Calculate balanced accuracy
TPR_weighted <- conf_matrix_weighted["yes", "yes"] / sum(conf_matrix_weighted[, "yes"])  # True Positive Rate
TNR_weighted <- conf_matrix_weighted["no", "no"] / sum(conf_matrix_weighted[, "no"])     # True Negative Rate
balanced_accuracy_weighted <- (TPR_weighted + TNR_weighted) / 2
# Print results
cat("Misclassification Rate for 'no' (weighted):", misclass_rate_no_weighted, "\n")
cat("Misclassification Rate for 'yes' (weighted):", misclass_rate_yes_weighted, "\n")
cat("Balanced Accuracy (weighted):", balanced_accuracy_weighted, "\n")
# Perform stepwise selection on the weighted logistic regression model
simplified_model <- step(weighted_logit_model, direction = "both")
# Summarize the simplified model
summary(simplified_model)
# Predict probabilities using the simplified model on the test set
test_data$simplified_prob <- predict(simplified_model, newdata = test_data, type = "response")
# Convert probabilities to class labels (default threshold = 0.5)
test_data$simplified_class <- ifelse(test_data$simplified_prob > 0.5, "yes", "no")
# Ensure confusion matrix includes both "no" and "yes"
conf_matrix_simplified <- table(
Predicted = factor(test_data$simplified_class, levels = c("no", "yes")),
Actual = factor(test_data$y, levels = c("no", "yes"))
)
# Re-run misclassification rate calculations
misclass_rate_no_simplified <- conf_matrix_simplified["yes", "no"] / sum(conf_matrix_simplified[, "no"])
misclass_rate_yes_simplified <- conf_matrix_simplified["no", "yes"] / sum(conf_matrix_simplified[, "yes"])
# Calculate balanced accuracy
TPR_simplified <- conf_matrix_simplified["yes", "yes"] / sum(conf_matrix_simplified[, "yes"])  # True Positive Rate
TNR_simplified <- conf_matrix_simplified["no", "no"] / sum(conf_matrix_simplified[, "no"])     # True Negative Rate
balanced_accuracy_simplified <- (TPR_simplified + TNR_simplified) / 2
# Print results
cat("Misclassification Rate for 'no' (simplified):", misclass_rate_no_simplified, "\n")
cat("Misclassification Rate for 'yes' (simplified):", misclass_rate_yes_simplified, "\n")
cat("Balanced Accuracy (simplified):", balanced_accuracy_simplified, "\n")
# Load required libraries
library(ISLR)
library(glmnet)
# Load the dataset
data(Khan)
# Structure of the dataset
str(Khan)
# Convert response to a factor
ytrain <- as.factor(Khan$ytrain)
# Fit multinomial model using cross-validation
fit_glmnet <- cv.glmnet(Khan$xtrain, ytrain, family = "multinomial")
# Plot cross-validation results
plot(fit_glmnet)
# Display the values of lambda.min and lambda.1se
cat("Lambda.min:", fit_glmnet$lambda.min, "\n")
cat("Lambda.1se:", fit_glmnet$lambda.1se, "\n")
# Extract coefficients for all groups at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
for (group in names(coefficients)) {
cat("\nCoefficients for group:", group, "\n")
# print(as.matrix(coefficients[[group]]))
}
# Identify relevant coefficients (non-zero) for each group
relevant_coefficients <- lapply(coefficients, function(coef_matrix) {
coef_matrix <- as.matrix(coef_matrix)
relevant <- coef_matrix[coef_matrix != 0, , drop = FALSE]
return(relevant)
})
# Print relevant coefficients
cat("\nRelevant coefficients for each group:\n")
print(relevant_coefficients)
# Ensure the variable "V2022" is included in the dataset and extract it
selected_variable <- "V2022"  # Replace with any other relevant variable if needed
# Convert the training data to a data frame for ggplot2
training_data <- as.data.frame(Khan$xtrain)
training_data$response <- as.factor(Khan$ytrain)  # Add response as a factor
# Generate the plot
library(ggplot2)
ggplot(training_data, aes_string(x = selected_variable, y = "response", color = "response")) +
geom_jitter(width = 0.2, height = 0.1, alpha = 0.6) +
labs(
title = paste("Distribution of", selected_variable, "by Response Group"),
x = selected_variable,
y = "Response (Groups)",
color = "Group"
) +
theme_minimal()
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, s = "lambda.1se", type = "response")
# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)
# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)
# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)
# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
# Set global options for all code chunks
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, tidy = TRUE)
# Load the data
d <- read.csv2("bank.csv")
# Exclude the variable 'duration'
d <- d[, !colnames(d) %in% "duration"]
# Convert target variable to a factor
d$y <- as.factor(d$y)
# Check class distribution
table(d$y)
head(d)
summary(d)
set.seed(123)
# Randomly select 3000 observations for training
train_indices <- sample(nrow(d), 3000)
train_data <- d[train_indices, ]
test_data <- d[-train_indices, ]
# Fit logistic regression model
logit_model <- glm(y ~ ., data = train_data, family = "binomial")
# Summary of the model
summary(logit_model)
# Predict probabilities for the test set
test_data$predicted_prob <- predict(logit_model, newdata = test_data,
type = "response")
# Convert probabilities to class labels (default threshold = 0.5)
test_data$predicted_class <- ifelse(test_data$predicted_prob > 0.5, "yes", "no")
# Create a confusion matrix
conf_matrix <- table(Predicted = test_data$predicted_class,
Actual = test_data$y)
# Calculate misclassification rate for each group
misclass_rate_no <- conf_matrix["yes", "no"] / sum(conf_matrix[, "no"])
misclass_rate_yes <- conf_matrix["no", "yes"] / sum(conf_matrix[, "yes"])
# Calculate balanced accuracy
TPR <- conf_matrix["yes", "yes"] / sum(conf_matrix[, "yes"]) # True Positive Rate
TNR <- conf_matrix["no", "no"] / sum(conf_matrix[, "no"]) # True Negative Rate
balanced_accuracy <- (TPR + TNR) / 2
# Print results
cat("Misclassification Rate for 'no':", misclass_rate_no, "\n")
cat("Misclassification Rate for 'yes':", misclass_rate_yes, "\n")
cat("Balanced Accuracy:", balanced_accuracy, "\n")
# Calculate weights
class_weights <- ifelse(train_data$y == "yes",
1 / sum(train_data$y == "yes"),
1 / sum(train_data$y == "no"))
# Normalize the weights
class_weights <- class_weights * length(train_data$y)
# Fit weighted logistic regression model
weighted_logit_model <- glm(y ~ ., data = train_data, family = "binomial",
weights = class_weights)
# Predict probabilities on the test set
test_data$weighted_prob <- predict(weighted_logit_model, newdata = test_data,
type = "response")
# Convert probabilities to class labels (default threshold = 0.5)
test_data$weighted_class <- ifelse(test_data$weighted_prob > 0.5, "yes", "no")
# Create a confusion matrix
conf_matrix_weighted <- table(Predicted = test_data$weighted_class,
Actual = test_data$y)
# Calculate misclassification rate for each group
misclass_rate_no_weighted <-
conf_matrix_weighted["yes", "no"] / sum(conf_matrix_weighted[, "no"])
misclass_rate_yes_weighted <-
conf_matrix_weighted["no", "yes"] / sum(conf_matrix_weighted[, "yes"])
# Calculate balanced accuracy
TPR_weighted <-
conf_matrix_weighted["yes", "yes"] / sum(conf_matrix_weighted[, "yes"])
TNR_weighted <-
conf_matrix_weighted["no", "no"] / sum(conf_matrix_weighted[, "no"])
balanced_accuracy_weighted <- (TPR_weighted + TNR_weighted) / 2
# Print results
cat("Misclassification Rate for 'no' (weighted):",
misclass_rate_no_weighted, "\n")
cat("Misclassification Rate for 'yes' (weighted):",
misclass_rate_yes_weighted, "\n")
cat("Balanced Accuracy (weighted):", balanced_accuracy_weighted, "\n")
# Perform stepwise selection on the weighted logistic regression model
simplified_model <- step(weighted_logit_model, direction = "both")
# Summarize the simplified model
summary(simplified_model)
# Predict probabilities using the simplified model on the test set
test_data$simplified_prob <- predict(simplified_model, newdata = test_data, type = "response")
# Convert probabilities to class labels (default threshold = 0.5)
test_data$simplified_class <- ifelse(test_data$simplified_prob > 0.5, "yes", "no")
# Ensure confusion matrix includes both "no" and "yes"
conf_matrix_simplified <- table(
Predicted = factor(test_data$simplified_class, levels = c("no", "yes")),
Actual = factor(test_data$y, levels = c("no", "yes"))
)
# Re-run misclassification rate calculations
misclass_rate_no_simplified <- conf_matrix_simplified["yes", "no"] / sum(conf_matrix_simplified[, "no"])
misclass_rate_yes_simplified <- conf_matrix_simplified["no", "yes"] / sum(conf_matrix_simplified[, "yes"])
# Calculate balanced accuracy
TPR_simplified <- conf_matrix_simplified["yes", "yes"] / sum(conf_matrix_simplified[, "yes"])  # True Positive Rate
TNR_simplified <- conf_matrix_simplified["no", "no"] / sum(conf_matrix_simplified[, "no"])     # True Negative Rate
balanced_accuracy_simplified <- (TPR_simplified + TNR_simplified) / 2
# Print results
cat("Misclassification Rate for 'no' (simplified):", misclass_rate_no_simplified, "\n")
cat("Misclassification Rate for 'yes' (simplified):", misclass_rate_yes_simplified, "\n")
cat("Balanced Accuracy (simplified):", balanced_accuracy_simplified, "\n")
# Load required libraries
library(ISLR)
library(glmnet)
# Load the dataset
data(Khan)
# Structure of the dataset
str(Khan)
# Convert response to a factor
ytrain <- as.factor(Khan$ytrain)
# Fit multinomial model using cross-validation
fit_glmnet <- cv.glmnet(Khan$xtrain, ytrain, family = "multinomial")
# Plot cross-validation results
plot(fit_glmnet)
# Display the values of lambda.min and lambda.1se
cat("Lambda.min:", fit_glmnet$lambda.min, "\n")
cat("Lambda.1se:", fit_glmnet$lambda.1se, "\n")
# Extract coefficients for all groups at lambda.1se
coefficients <- coef(fit_glmnet, s = "lambda.1se")
# Display coefficients for each group
for (group in names(coefficients)) {
cat("\nCoefficients for group:", group, "\n")
# print(as.matrix(coefficients[[group]]))
}
# Identify relevant coefficients (non-zero) for each group
relevant_coefficients <- lapply(coefficients, function(coef_matrix) {
coef_matrix <- as.matrix(coef_matrix)
relevant <- coef_matrix[coef_matrix != 0, , drop = FALSE]
return(relevant)
})
# Print relevant coefficients
cat("\nRelevant coefficients for each group:\n")
print(relevant_coefficients)
# Ensure the variable "V2022" is included in the dataset and extract it
selected_variable <- "V2022"  # Replace with any other relevant variable if needed
# Convert the training data to a data frame for ggplot2
training_data <- as.data.frame(Khan$xtrain)
training_data$response <- as.factor(Khan$ytrain)  # Add response as a factor
# Generate the plot
library(ggplot2)
ggplot(training_data, aes_string(x = selected_variable, y = "response", color = "response")) +
geom_jitter(width = 0.2, height = 0.1, alpha = 0.6) +
labs(
title = paste("Distribution of", selected_variable, "by Response Group"),
x = selected_variable,
y = "Response (Groups)",
color = "Group"
) +
theme_minimal()
# Predict probabilities for each class on the test data
predicted_probs <- predict(fit_glmnet, newx = Khan$xtest, s = "lambda.1se", type = "response")
# Select the predicted class for each observation
predicted_classes <- apply(predicted_probs, 1, which.max)
# Create a confusion table
confusion_table <- table(Predicted = predicted_classes, Actual = Khan$ytest)
# Calculate the misclassification error
misclassification_error <- 1 - sum(diag(confusion_table)) / sum(confusion_table)
# Print the results
cat("Confusion Table:\n")
print(confusion_table)
cat("\nMisclassification Error:", misclassification_error, "\n")
